
@article{baayen_mixed-effects_2008,
	title = {Mixed-effects modeling with crossed random effects for subjects and items},
	volume = {59},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X07001398},
	doi = {10.1016/j.jml.2007.12.005},
	abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed.},
	number = {4},
	journal = {Special Issue: Emerging Data Analysis},
	author = {Baayen, R.H. and Davidson, D.J. and Bates, D.M.},
	month = nov,
	year = {2008},
	keywords = {By-item, By-subject, Crossed random effects, Mixed-effects models, Quasi-F, notion},
	pages = {390--412},
}

@incollection{singmann_introduction_2019,
	title = {An {Introduction} to {Mixed} {Models} for {Experimental} {Psychology}},
	isbn = {978-0-429-31840-5},
	booktitle = {New {Methods} in {Cognitive} {Psychology}},
	author = {Singmann, Henrik and Kellen, David},
	month = oct,
	year = {2019},
	doi = {10.4324/9780429318405-2},
	note = {Journal Abbreviation: New Methods in Cognitive Psychology},
	keywords = {notion},
	pages = {4--31},
}

@article{krzywinski_nested_2014,
	title = {Nested designs},
	volume = {11},
	issn = {1548-7091},
	doi = {10.1038/nmeth.3137},
	abstract = {For studies with hierarchical noise sources, use a nested analysis of variance approach.},
	number = {10},
	journal = {Nature Methods},
	author = {Krzywinski, Martin and Altman, Naomi and Blainey, Paul},
	year = {2014},
	keywords = {notion},
	pages = {977--978},
}

@article{andersen_bottom-up_2012,
	title = {Bottom-{Up} {Biases} in {Feature}-{Selective} {Attention}},
	volume = {32},
	doi = {10.1523/JNEUROSCI.1767-12.2012},
	journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
	author = {Andersen, Søren and Müller, Matthias and Martinovic, Jasna},
	month = nov,
	year = {2012},
	keywords = {notion},
	pages = {16953--8},
}

@article{stadler_statistical_2021,
	title = {Statistical modeling of dynamic eye-tracking experiments: {Relative} importance of visual stimulus elements for gaze behavior in the multi-group case},
	volume = {53},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-021-01576-8},
	doi = {10.3758/s13428-021-01576-8},
	abstract = {This paper presents a model that allows group comparisons of gaze behavior while watching dynamic video stimuli. The model is based on the approach of Coutrot and Guyader (2017) and allows linear combinations of feature maps to form a master saliency map. The feature maps in the model are, for example, the dynamically salient contents of a video stimulus or predetermined areas of interest. The model takes into account temporal aspects of the stimuli, which is a crucial difference to other common models. The multi-group extension of the model introduced here allows to obtain relative importance plots, which visualize the effect of a specific feature of a stimulus on the attention and visual behavior for two or more experimental groups. These plots are interpretable summaries of data with high spatial and temporal resolution. This approach differs from many common methods for comparing gaze behavior between natural groups, which usually only include single-dimensional features such as the duration of fixation on a particular part of the stimulus. The method is illustrated by contrasting a sample of a group of persons with particularly high cognitive abilities (high achievement on IQ tests) with a control group on a psycholinguistic task on the conceptualization of motion events. In the example, we find no substantive differences in relative importance, but more exploratory gaze behavior in the highly gifted group. The code, videos, and eye-tracking data we used for this study are available online.},
	number = {6},
	journal = {Behavior Research Methods},
	author = {Stadler, Mara and Doebler, Philipp and Mertins, Barbara and Delucchi Danhier, Renate},
	month = dec,
	year = {2021},
	keywords = {notion},
	pages = {2650--2667},
}

@article{schielzeth_conclusions_2009,
	title = {Conclusions beyond support: overconfident estimates in mixed models},
	volume = {20},
	issn = {1045-2249},
	url = {https://doi.org/10.1093/beheco/arn145},
	doi = {10.1093/beheco/arn145},
	abstract = {Mixed-effect models are frequently used to control for the nonindependence of data points, for example, when repeated measures from the same individuals are available. The aim of these models is often to estimate fixed effects and to test their significance. This is usually done by including random intercepts, that is, intercepts that are allowed to vary between individuals. The widespread belief is that this controls for all types of pseudoreplication within individuals. Here we show that this is not the case, if the aim is to estimate effects that vary within individuals and individuals differ in their response to these effects. In these cases, random intercept models give overconfident estimates leading to conclusions that are not supported by the data. By allowing individuals to differ in the slopes of their responses, it is possible to account for the nonindependence of data points that pseudoreplicate slope information. Such random slope models give appropriate standard errors and are easily implemented in standard statistical software. Because random slope models are not always used where they are essential, we suspect that many published findings have too narrow confidence intervals and a substantially inflated type I error rate. Besides reducing type I errors, random slope models have the potential to reduce residual variance by accounting for between-individual variation in slopes, which makes it easier to detect treatment effects that are applied between individuals, hence reducing type II errors as well.},
	number = {2},
	urldate = {2022-12-10},
	journal = {Behavioral Ecology},
	author = {Schielzeth, Holger and Forstmeier, Wolfgang},
	month = mar,
	year = {2009},
	keywords = {notion},
	pages = {416--420},
}

@article{schielzeth_simple_2010,
	title = {Simple means to improve the interpretability of regression coefficients},
	volume = {1},
	issn = {2041-210X},
	url = {https://doi.org/10.1111/j.2041-210X.2010.00012.x},
	doi = {10.1111/j.2041-210X.2010.00012.x},
	abstract = {Summary 1. Linear regression models are an important statistical tool in evolutionary and ecological studies. Unfortunately, these models often yield some uninterpretable estimates and hypothesis tests, especially when models contain interactions or polynomial terms. Furthermore, the standard errors for treatment groups, although often of interest for including in a publication, are not directly available in a standard linear model. 2. Centring and standardization of input variables are simple means to improve the interpretability of regression coefficients. Further, refitting the model with a slightly modified model structure allows extracting the appropriate standard errors for treatment groups directly from the model. 3. Centring will make main effects biologically interpretable even when involved in interactions and thus avoids the potential misinterpretation of main effects. This also applies to the estimation of linear effects in the presence of polynomials. Categorical input variables can also be centred and this sometimes assists interpretation. 4. Standardization (z-transformation) of input variables results in the estimation of standardized slopes or standardized partial regression coefficients. Standardized slopes are comparable in magnitude within models as well as between studies. They have some advantages over partial correlation coefficients and are often the more interesting standardized effect size. 5. The thoughtful removal of intercepts or main effects allows extracting treatment means or treatment slopes and their appropriate standard errors directly from a linear model. This provides a simple alternative to the more complicated calculation of standard errors from contrasts and main effects. 6. The simple methods presented here put the focus on parameter estimation (point estimates as well as confidence intervals) rather than on significance thresholds. They allow fitting complex, but meaningful models that can be concisely presented and interpreted. The presented methods can also be applied to generalised linear models (GLM) and linear mixed models.},
	number = {2},
	urldate = {2022-10-11},
	journal = {Methods in Ecology and Evolution},
	author = {Schielzeth, Holger},
	month = jun,
	year = {2010},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {confidence intervals, generalized linear models, interaction terms, null hypothesis testing, partial correlation coefficients, partial regression coefficients, standard errors, standardized effects sizes, notion},
	pages = {103--113},
	annote = {https://doi.org/10.1111/j.2041-210X.2010.00012.x},
}

@article{chun_contextual_1998,
	title = {Contextual {Cueing}: {Implicit} {Learning} and {Memory} of {Visual} {Context} {Guides} {Spatial} {Attention}},
	volume = {36},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/S0010028598906818},
	doi = {10.1006/cogp.1998.0681},
	abstract = {Global context plays an important, but poorly understood, role in visual tasks. This study demonstrates that a robust memory for visual context exists to guide spatial attention. Global context was operationalized as the spatial layout of objects in visual search displays. Half of the configurations were repeated across blocks throughout the entire session, and targets appeared within consistent locations in these arrays. Targets appearing in learned configurations were detected more quickly. This newly discovered form of search facilitation is termed contextual cueing. Contextual cueing is driven by incidentally learned associations between spatial configurations (context) and target locations. This benefit was obtained despite chance performance for recognizing the configurations, suggesting that the memory for context was implicit. The results show how implicit learning and memory of visual context can guide spatial attention towards task-relevant aspects of a scene.},
	number = {1},
	journal = {Cognitive Psychology},
	author = {Chun, Marvin M. and Jiang, Yuhong},
	month = jun,
	year = {1998},
	keywords = {notion},
	pages = {28--71},
}

@article{pollmann_working_2019,
	title = {Working memory dependence of spatial contextual cueing for visual search},
	volume = {110},
	issn = {0007-1269},
	url = {https://doi.org/10.1111/bjop.12311},
	doi = {10.1111/bjop.12311},
	abstract = {When spatial stimulus configurations repeat in visual search, a search facilitation, resulting in shorter search times, can be observed that is due to incidental learning. This contextual cueing effect appears to be rather implicit, uncorrelated with observers? explicit memory of display configurations. Nevertheless, as I review here, this search facilitation due to contextual cueing depends on visuospatial working memory resources, and it disappears when visuospatial working memory is loaded by a concurrent delayed match to sample task. However, the search facilitation immediately recovers for displays learnt under visuospatial working memory load when this load is removed in a subsequent test phase. Thus, latent learning of visuospatial configurations does not depend on visuospatial working memory, but the expression of learning, as memory-guided search in repeated displays, does. This working memory dependence has also consequences for visual search with foveal vision loss, where top-down controlled visual exploration strategies pose high demands on visuospatial working memory, in this way interfering with memory-guided search in repeated displays. Converging evidence for the contribution of working memory to contextual cueing comes from neuroimaging data demonstrating that distinct cortical areas along the intraparietal sulcus as well as more ventral parieto-occipital cortex are jointly activated by visual working memory and contextual cueing.},
	number = {2},
	urldate = {2022-10-13},
	journal = {British Journal of Psychology},
	author = {Pollmann, Stefan},
	month = may,
	year = {2019},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {contextual cueing, visuospatial, working memory, notion},
	pages = {372--380},
	annote = {https://doi.org/10.1111/bjop.12311},
}

@article{chun_contextual_2000,
	title = {Contextual cueing of visual attention},
	volume = {4},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661300014765},
	doi = {10.1016/S1364-6613(00)01476-5},
	abstract = {Visual context information constrains what to expect and where to look, facilitating search for and recognition of objects embedded in complex displays. This article reviews a new paradigm called contextual cueing, which presents well-defined, novel visual contexts and aims to understand how contextual information is learned and how it guides the deployment of visual attention. In addition, the contextual cueing task is well suited to the study of the neural substrate of contextual learning. For example, amnesic patients with hippocampal damage are impaired in their learning of novel contextual information, even though learning in the contextual cueing task does not appear to rely on conscious retrieval of contextual memory traces. We argue that contextual information is important because it embodies invariant properties of the visual environment such as stable spatial layout information as well as object covariation information. Sensitivity to these statistical regularities allows us to interact more effectively with the visual world.},
	number = {5},
	journal = {Trends in Cognitive Sciences},
	author = {Chun, Marvin M.},
	month = may,
	year = {2000},
	keywords = {Perceptual learning, Attention, Context, Hippocampus, Implicit learning, Implicit memory, Search, notion},
	pages = {170--178},
}

@article{makovski_contextual_2010,
	title = {Contextual cost: {When} a visual-search target is not where it should be},
	volume = {63},
	issn = {1747-0218},
	url = {https://doi.org/10.1080/17470210903281590},
	doi = {10.1080/17470210903281590},
	abstract = {Visual search is often facilitated when the search display occasionally repeats, revealing a contextual-cueing effect. According to the associative-learning account, contextual cueing arises from associating the display configuration with the target location. However, recent findings emphasizing the importance of local context near the target have given rise to the possibility that low-level repetition priming may account for the contextual-cueing effect. This study distinguishes associative learning from local repetition priming by testing whether search is directed toward a target's expected location, even when the target is relocated. After participants searched for a T among Ls in displays that repeated 24 times, they completed a transfer session where the target was relocated locally to a previously blank location (Experiment 1) or to an adjacent distractor location (Experiment 2). Results revealed that contextual cueing decreased as the target appeared farther away from its expected location, ultimately resulting in a contextual cost when the target swapped locations with a local distractor. We conclude that target predictability is a key factor in contextual cueing.},
	number = {2},
	urldate = {2022-10-13},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Makovski, Tal and Jiang, Yuhong V.},
	month = feb,
	year = {2010},
	note = {Publisher: SAGE Publications},
	keywords = {notion},
	pages = {216--225},
	annote = {doi: 10.1080/17470210903281590},
}

@article{mack_object_2011,
	title = {Object co-occurrence serves as a contextual cue to guide and facilitate visual search in a natural viewing environment},
	volume = {11},
	issn = {1534-7362},
	url = {https://doi.org/10.1167/11.9.9},
	doi = {10.1167/11.9.9},
	abstract = {There is accumulating evidence that scene context can guide and facilitate visual search (e.g., A. Torralba, A. Oliva, M. S. Castelhano, \&amp; J. M. Henderson, 2006). Previous studies utilized stimuli of restricted size, a fixed head position, and context defined by the global spatial configuration of the scene. Thus, it is unknown whether similar effects generalize to natural viewing environments and to context defined by local object co-occurrence. Here, with a mobile eye tracker, we investigated the effects of object co-occurrence on search performance under naturalistic conditions. Observers searched for low-visibility target objects on tables cluttered with everyday objects. Targets were either located adjacent to larger, more visible “cue” objects that they regularly co-occurred in natural scenes (expected condition) or elsewhere in the display, surrounded by unrelated objects (unexpected condition). Mean search times were shorter for targets at expected locations as compared to unexpected locations. Additionally, context guided eye movements, as more fixations were directed toward cue objects than other non-target objects, particularly when the cue was contextually relevant to the current search target. These results could not be accounted for by image saliency models. Thus, we conclude that object co-occurrence can serve as a contextual cue to facilitate search and guide eye movements in natural environments.},
	number = {9},
	urldate = {2022-10-13},
	journal = {Journal of Vision},
	author = {Mack, Stephen C. and Eckstein, Miguel P.},
	month = aug,
	year = {2011},
	keywords = {notion},
	pages = {9--9},
}

@article{schlagbauer_awareness_2012,
	title = {Awareness in contextual cueing of visual search as measured with concurrent access- and phenomenal-consciousness tasks},
	volume = {12},
	issn = {1534-7362},
	url = {https://doi.org/10.1167/12.11.25},
	doi = {10.1167/12.11.25},
	abstract = {In visual search, context information can serve as a cue to guide attention to the target location. When observers repeatedly encounter displays with identical target-distractor arrangements, reaction times (RTs) are faster for repeated relative to nonrepeated displays, the latter containing novel configurations. This effect has been termed “contextual cueing.” The present study asked whether information about the target location in repeated displays is “explicit” (or “conscious”) in nature. To examine this issue, observers performed a test session (after an initial training phase in which RTs to repeated and nonrepeated displays were measured) in which the search stimuli were presented briefly and terminated by visual masks; following this, observers had to make a target localization response (with accuracy as the dependent measure) and indicate their visual experience and confidence associated with the localization response. The data were examined at the level of individual displays, i.e., in terms of whether or not a repeated display actually produced contextual cueing. The results were that (a) contextual cueing was driven by only a very small number of about four actually learned configurations; (b) localization accuracy was increased for learned relative to nonrepeated displays; and (c) both consciousness measures were enhanced for learned compared to nonrepeated displays. It is concluded that contextual cueing is driven by only a few repeated displays and the ability to locate the target in these displays is associated with increased visual experience.},
	number = {11},
	urldate = {2022-10-13},
	journal = {Journal of Vision},
	author = {Schlagbauer, Bernhard and Müller, Hermann J. and Zehetleitner, Michael and Geyer, Thomas},
	month = oct,
	year = {2012},
	keywords = {notion},
	pages = {25--25},
}

@article{conci_limitations_2011,
	title = {Limitations of perceptual segmentation on contextual cueing in visual search},
	volume = {19},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/13506285.2010.518574},
	doi = {10.1080/13506285.2010.518574},
	number = {2},
	journal = {Visual Cognition},
	author = {Conci, Markus and von Mühlenen, Adrian},
	month = feb,
	year = {2011},
	note = {Publisher: Routledge},
	keywords = {notion},
	pages = {203--233},
	annote = {doi: 10.1080/13506285.2010.518574},
}

@article{brockmole_recognition_2006,
	title = {Recognition and attention guidance during contextual cueing in real-world scenes: {Evidence} from eye movements},
	volume = {59},
	issn = {1747-0218},
	url = {https://doi.org/10.1080/17470210600665996},
	doi = {10.1080/17470210600665996},
	number = {7},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Brockmole, James   R. and Henderson, John   M.},
	month = jul,
	year = {2006},
	note = {Publisher: Routledge},
	keywords = {notion},
	pages = {1177--1187},
	annote = {doi: 10.1080/17470210600665996},
}

@article{manginelli_misleading_2009,
	title = {Misleading contextual cues: {How} do they affect visual search?},
	volume = {73},
	issn = {1430-2772},
	url = {https://doi.org/10.1007/s00426-008-0211-1},
	doi = {10.1007/s00426-008-0211-1},
	abstract = {Contextual cueing occurs when repetitions of the distractor configuration are implicitly learned. This implicit learning leads to faster search times in repeated displays. Here, we investigated how search adapts to a change of the target location in old displays from a consistent location in the learning phase to a consistent new location in the transfer phase. In agreement with the literature, contextual cueing was accompanied by fewer fixations, a more efficient scan path and, specifically, an earlier onset of a monotonic gaze approach phase towards the target location in repeated displays. When the repeated context was no longer predictive of the old target location, search times and number of fixations for old displays increased to the level of novel displays. Along with this, scan paths for old and new displays became equally efficient. After the target location change, there was a bias of exploration towards the old target location, which soon disappeared. Thus, change of implicitly learned spatial relations between target and distractor configuration eliminated the advantageous effects of contextual cueing, but did not lead to a lasting impairment of search in repeated displays relative to novel displays.},
	number = {2},
	journal = {Psychological Research},
	author = {Manginelli, Angela A. and Pollmann, Stefan},
	month = mar,
	year = {2009},
	keywords = {notion},
	pages = {212--221},
}

@article{zellin_here_2013,
	title = {Here {Today}, {Gone} {Tomorrow} – {Adaptation} to {Change} in {Memory}-{Guided} {Visual} {Search}},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0059466},
	doi = {10.1371/journal.pone.0059466},
	abstract = {Visual search for a target object can be facilitated by the repeated presentation of an invariant configuration of nontargets (‘contextual cueing’). Here, we tested adaptation of learned contextual associations after a sudden, but permanent, relocation of the target. After an initial learning phase targets were relocated within their invariant contexts and repeatedly presented at new locations, before they returned to the initial locations. Contextual cueing for relocated targets was neither observed after numerous presentations nor after insertion of an overnight break. Further experiments investigated whether learning of additional, previously unseen context-target configurations is comparable to adaptation of existing contextual associations to change. In contrast to the lack of adaptation to changed target locations, contextual cueing developed for additional invariant configurations under identical training conditions. Moreover, across all experiments, presenting relocated targets or additional contexts did not interfere with contextual cueing of initially learned invariant configurations. Overall, the adaptation of contextual memory to changed target locations was severely constrained and unsuccessful in comparison to learning of an additional set of contexts, which suggests that contextual cueing facilitates search for only one repeated target location.},
	number = {3},
	urldate = {2022-10-13},
	journal = {PLOS ONE},
	author = {Zellin, Martina and Conci, Markus and Mühlenen, Adrian von and Müller, Hermann J.},
	month = mar,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Learning, Adults, Analysis of variance, Experimental design, Sensory cues, Target detection, Vision, Visual acuity, notion},
	pages = {e59466},
	file = {Full Text PDF:/Users/luca/Zotero/storage/9YXGMQ6T/Zellin et al. - 2013 - Here Today, Gone Tomorrow – Adaptation to Change i.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/JCTRQ87E/citation.html:text/html},
}

@article{makovski_meaning_2018,
	title = {Meaning in learning: {Contextual} cueing relies on objects’ visual features and not on objects’ meaning},
	volume = {46},
	issn = {1532-5946},
	url = {https://doi.org/10.3758/s13421-017-0745-9},
	doi = {10.3758/s13421-017-0745-9},
	abstract = {People easily learn regularities embedded in the environment and utilize them to facilitate visual search. Using images of real-world objects, it has been recently shown that this learning, termed contextual cueing (CC), occurs even in complex, heterogeneous environments, but only when the same distractors are repeated at the same locations. Yet it is not clear what exactly is being learned under these conditions: the visual features of the objects or their meaning. In this study, Experiment 1 demonstrated that meaning is not necessary for this type of learning, as a similar pattern of results was found even when the objects’ meaning was largely removed. Experiments 2 and 3 showed that after learning meaningful objects, CC was not diminished by a manipulation that distorted the objects’ meaning but preserved most of their visual properties. By contrast, CC was eliminated when the learned objects were replaced with different category exemplars that preserved the objects’ meaning but altered their visual properties. Together, these data strongly suggest that the acquired context that facilitates real-world objects search relies primarily on the visual properties and the spatial locations of the objects, but not on their meaning.},
	number = {1},
	journal = {Memory \& Cognition},
	author = {Makovski, Tal},
	month = jan,
	year = {2018},
	keywords = {notion},
	pages = {58--67},
}

@article{barr_random_2013,
	title = {Random effects structure for testing interactions in linear mixed-effects models},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00328},
	journal = {Frontiers in Psychology},
	author = {Barr, Dale},
	year = {2013},
	keywords = {notion},
}

@article{heisig_why_2019,
	title = {Why {You} {Should} {Always} {Include} a {Random} {Slope} for the {Lower}-{Level} {Variable} {Involved} in a {Cross}-{Level} {Interaction}},
	volume = {35},
	issn = {0266-7215},
	url = {https://doi.org/10.1093/esr/jcy053},
	doi = {10.1093/esr/jcy053},
	abstract = {Mixed-effects multilevel models are often used to investigate cross-level interactions, a specific type of context effect that may be understood as an upper-level variable moderating the association between a lower-level predictor and the outcome. We argue that multilevel models involving cross-level interactions should always include random slopes on the lower-level components of those interactions. Failure to do so will usually result in severely anti-conservative statistical inference. We illustrate the problem with extensive Monte Carlo simulations and examine its practical relevance by studying 30 prototypical cross-level interactions with European Social Survey data for 28 countries. In these empirical applications, introducing a random slope term reduces the absolute t-ratio of the cross-level interaction term by 31 per cent or more in three quarters of cases, with an average reduction of 42 per cent. Many practitioners seem to be unaware of these issues. Roughly half of the cross-level interaction estimates published in the European Sociological Review between 2011 and 2016 are based on models that omit the crucial random slope term. Detailed analysis of the associated test statistics suggests that many of the estimates would not reach conventional thresholds for statistical significance in correctly specified models that include the random slope. This raises the question how much robust evidence of cross-level interactions sociology has actually produced over the past decades.},
	number = {2},
	urldate = {2022-10-13},
	journal = {European Sociological Review},
	author = {Heisig, Jan Paul and Schaeffer, Merlin},
	month = apr,
	year = {2019},
	keywords = {notion},
	pages = {258--279},
}

@book{hox_multilevel_2017,
	address = {New York},
	edition = {3},
	title = {Multilevel {Analysis}: {Techniques} and {Applications}},
	isbn = {978-1-315-65098-2},
	shorttitle = {Multilevel {Analysis}},
	abstract = {Applauded for its clarity, this accessible introduction helps readers apply multilevel techniques to their research. The book also includes advanced extensions, making it useful as both an introduction for students and as a reference for researchers. Basic models and examples are discussed in nontechnical terms with an emphasis on understanding the methodological and statistical issues involved in using these models. The estimation and interpretation of multilevel models is demonstrated using realistic examples from various disciplines including psychology, education, public health, and sociology. Readers are introduced to a general framework on multilevel modeling which covers both observed and latent variables in the same model, while most other books focus on observed variables. In addition, Bayesian estimation is introduced and applied using accessible software.},
	publisher = {Routledge},
	author = {Hox, Joop and Moerbeek, Mirjam and Schoot, Rens van de},
	month = sep,
	year = {2017},
	doi = {10.4324/9781315650982},
	keywords = {notion},
}

@article{gurka_extending_2006,
	title = {Extending the {Box}–{Cox} transformation to the linear mixed model},
	volume = {169},
	issn = {0964-1998},
	url = {https://doi.org/10.1111/j.1467-985X.2005.00391.x},
	doi = {10.1111/j.1467-985X.2005.00391.x},
	abstract = {Summary.? For a univariate linear model, the Box?Cox method helps to choose a response transformation to ensure the validity of a Gaussian distribution and related assumptions. The desire to extend the method to a linear mixed model raises many vexing questions. Most importantly, how do the distributions of the two sources of randomness (pure error and random effects) interact in determining the validity of assumptions? For an otherwise valid model, we prove that the success of a transformation may be judged solely in terms of how closely the total error follows a Gaussian distribution. Hence the approach avoids the complexity of separately evaluating pure errors and random effects. The extension of the transformation to the mixed model requires an exploration of its potential effect on estimation and inference of the model parameters. Analysis of longitudinal pulmonary function data and Monte Carlo simulations illustrate the methodology discussed.},
	number = {2},
	urldate = {2022-10-15},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gurka, Matthew J. and Edwards, Lloyd J. and Muller, Keith E. and Kupper, Lawrence L.},
	month = mar,
	year = {2006},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {Linear mixed model, Longitudinal data, Lung function, Normality, Random effects, Transformation, notion},
	pages = {273--288},
	annote = {https://doi.org/10.1111/j.1467-985X.2005.00391.x},
}

@article{thai_comparison_2013,
	title = {A comparison of bootstrap approaches for estimating uncertainty of parameters in linear mixed-effects models},
	volume = {12},
	doi = {10.1002/pst.1561},
	journal = {Pharmaceutical statistics},
	author = {Thai, Hoai-Thu and Mentré, France and Holford, Nick and Veyrat-Follet, Christine and Comets, Emmanuelle},
	month = may,
	year = {2013},
	keywords = {notion},
}

@article{peirce_psychopy2_2019,
	title = {{PsychoPy2}: {Experiments} in behavior made easy},
	volume = {51},
	issn = {1554-3528},
	shorttitle = {{PsychoPy2}},
	url = {https://doi.org/10.3758/s13428-018-01193-y},
	doi = {10.3758/s13428-018-01193-y},
	abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
	language = {en},
	number = {1},
	urldate = {2022-10-17},
	journal = {Behavior Research Methods},
	author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and Höchenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindeløv, Jonas Kristoffer},
	month = feb,
	year = {2019},
	keywords = {Psychology, Experiment, Open science, Open-source, Reaction time, Software, Timing, notion},
	pages = {195--203},
	file = {Full Text PDF:/Users/luca/Zotero/storage/JHP9DQRN/Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf:application/pdf},
}

@article{peirce_psychopy2_2019-1,
	title = {{PsychoPy2}: {Experiments} in behavior made easy},
	volume = {51},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-018-01193-y},
	doi = {10.3758/s13428-018-01193-y},
	abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
	number = {1},
	journal = {Behavior Research Methods},
	author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and Höchenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindeløv, Jonas Kristoffer},
	month = feb,
	year = {2019},
	keywords = {notion},
	pages = {195--203},
}

@misc{r_core_team_r_2022,
	address = {Vienna, Austria},
	title = {R: {A} language and   environment for statistical computing},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {R Core Team},
	year = {2022},
}

@misc{rstudio_team_rstudio_2022,
	address = {Boston, MA},
	title = {{RStudio}: {Integrated} {Development} {Environment}   for {R}},
	url = {http://www.rstudio.com/.},
	publisher = {RStudio, PBC},
	author = {RStudio Team},
	year = {2022},
}

@article{wickham_welcome_2019,
	title = {Welcome to the {Tidyverse}},
	volume = {4},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.01686},
	doi = {10.21105/joss.01686},
	abstract = {Wickham et al., (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686},
	language = {en},
	number = {43},
	urldate = {2022-10-17},
	journal = {Journal of Open Source Software},
	author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
	month = nov,
	year = {2019},
	pages = {1686},
	file = {Full Text PDF:/Users/luca/Zotero/storage/ZAHPL6ML/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/4PA3VSPY/joss.html:text/html},
}

@misc{wickham_dplyr_2022,
	title = {dplyr: {A} {Grammar} of {Data} {Manipulation}},
	copyright = {MIT + file LICENSE},
	shorttitle = {dplyr},
	url = {https://CRAN.R-project.org/package=dplyr},
	abstract = {A fast, consistent tool for working with data frame like objects, both in memory and out of memory.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and François, Romain and Henry, Lionel and Müller, Kirill},
	month = sep,
	year = {2022},
	keywords = {Databases, ModelDeployment, Epidemiology},
}

@misc{wickham_readr_2022,
	title = {readr: {Read} {Rectangular} {Text} {Data}},
	copyright = {MIT + file LICENSE},
	shorttitle = {readr},
	url = {https://CRAN.R-project.org/package=readr},
	abstract = {The goal of 'readr' is to provide a fast and friendly way to read rectangular data (like 'csv', 'tsv', and 'fwf'). It is designed to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and Hester, Jim and Francois, Romain and Bryan, Jennifer and Bearrows, Shelby and Jylänki, Jylänki and Jørgensen, Mikkel},
	month = oct,
	year = {2022},
}

@misc{henry_purrr_2022,
	title = {purrr: {Functional} {Programming} {Tools}},
	copyright = {GPL-3 {\textbar} file LICENSE},
	shorttitle = {purrr},
	url = {https://CRAN.R-project.org/package=purrr},
	abstract = {A complete and consistent functional programming toolkit for R.},
	urldate = {2022-10-17},
	author = {Henry, Lionel and Wickham, Hadley},
	month = oct,
	year = {2022},
}

@misc{wickham_tidyr_2022,
	title = {tidyr: {Tidy} {Messy} {Data}},
	copyright = {MIT + file LICENSE},
	shorttitle = {tidyr},
	url = {https://CRAN.R-project.org/package=tidyr},
	abstract = {Tools to help to create tidy data, where each column is a variable, each row is an observation, and each cell contains a single value. 'tidyr' contains tools for changing the shape (pivoting) and hierarchy (nesting and 'unnesting') of a dataset, turning deeply nested lists into rectangular data frames ('rectangling'), and extracting values out of string columns. It also includes tools for working with missing values (both implicit and explicit).},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and Girlich, Maximilian},
	month = sep,
	year = {2022},
	keywords = {MissingData},
}

@misc{wickham_stringr_2022,
	title = {stringr: {Simple}, {Consistent} {Wrappers} for {Common} {String} {Operations}},
	copyright = {GPL-2 {\textbar} file LICENSE},
	shorttitle = {stringr},
	url = {https://CRAN.R-project.org/package=stringr},
	abstract = {A consistent, simple and easy to use set of wrappers around the fantastic 'stringi' package. All function and argument names (and positions) are consistent, all functions deal with "NA"'s and zero length vectors in the same way, and the output from one function is easy to feed into the input of another.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley},
	month = aug,
	year = {2022},
}

@misc{wickham_forcats_2022,
	title = {forcats: {Tools} for {Working} with {Categorical} {Variables} ({Factors})},
	copyright = {MIT + file LICENSE},
	shorttitle = {forcats},
	url = {https://CRAN.R-project.org/package=forcats},
	abstract = {Helpers for reordering factor levels (including moving specified levels to front, ordering by first appearance, reversing, and randomly shuffling), and tools for modifying factor levels (including collapsing rare levels into other, 'anonymising', and manually 'recoding').},
	urldate = {2022-10-17},
	author = {Wickham, Hadley},
	month = aug,
	year = {2022},
}

@misc{spinu_lubridate_2021,
	title = {lubridate: {Make} {Dealing} with {Dates} a {Little} {Easier}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {lubridate},
	url = {https://CRAN.R-project.org/package=lubridate},
	abstract = {Functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. The 'lubridate' package has a consistent and memorable syntax that makes working with dates easy and fun. Parts of the 'CCTZ' source code, released under the Apache 2.0 License, are included in this package. See {\textless}https://github.com/google/cctz{\textgreater} for more details.},
	urldate = {2022-10-17},
	author = {Spinu, Vitalie and Grolemund, Garrett and Wickham, Hadley and Vaughan, Davis and Lyttle, Ian and Costigan, Imanuel and Law, Jason and Mitarotonda, Doug and Larmarange, Joseph and Boiser, Jonathan and Lee, Chel Hee},
	month = oct,
	year = {2021},
	keywords = {TimeSeries, ReproducibleResearch},
}

@misc{kuhn_tidymodels_2022,
	title = {tidymodels: {Easily} {Install} and {Load} the '{Tidymodels}' {Packages}},
	copyright = {MIT + file LICENSE},
	shorttitle = {tidymodels},
	url = {https://CRAN.R-project.org/package=tidymodels},
	abstract = {The tidy modeling "verse" is a collection of packages for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.},
	urldate = {2022-10-17},
	author = {Kuhn, Max and Wickham, Hadley},
	month = jul,
	year = {2022},
	keywords = {MachineLearning},
}

@misc{kuhn_multilevelmod_2022,
	title = {multilevelmod: {Model} {Wrappers} for {Multi}-{Level} {Models}},
	copyright = {MIT + file LICENSE},
	shorttitle = {multilevelmod},
	url = {https://CRAN.R-project.org/package=multilevelmod},
	abstract = {Bindings for hierarchical regression models for use with the 'parsnip' package. Models include longitudinal generalized linear models (Liang and Zeger, 1986) {\textless}doi:10.1093/biomet/73.1.13{\textgreater}, and mixed-effect models (Pinheiro and Bates) {\textless}doi:10.1007/978-1-4419-0318-1\_1{\textgreater}.},
	urldate = {2022-10-17},
	author = {Kuhn, Max and Frick, Hannah and RStudio},
	month = jun,
	year = {2022},
}

@misc{bates_lme4_2022,
	title = {lme4: {Linear} {Mixed}-{Effects} {Models} using '{Eigen}' and {S4}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {lme4},
	url = {https://CRAN.R-project.org/package=lme4},
	abstract = {Fit linear and generalized linear mixed-effects models. The models and their components are represented using S4 classes and methods. The core computational algorithms are implemented using the 'Eigen' C++ library for numerical linear algebra and 'RcppEigen' "glue".},
	urldate = {2022-10-17},
	author = {Bates, Douglas and Maechler, Martin and Bolker, Ben and Walker, Steven and Christensen, Rune Haubo Bojesen and Singmann, Henrik and Dai, Bin and Scheipl, Fabian and Grothendieck, Gabor and Green, Peter and Fox, John and Bauer, Alexander and Krivitsky, Pavel N.},
	month = jul,
	year = {2022},
	keywords = {Psychometrics, Econometrics, Environmetrics, SpatioTemporal},
}

@misc{bolker_broommixed_2022,
	title = {broom.mixed: {Tidying} {Methods} for {Mixed} {Models}},
	copyright = {GPL-3},
	shorttitle = {broom.mixed},
	url = {https://CRAN.R-project.org/package=broom.mixed},
	abstract = {Convert fitted objects from various R mixed-model packages into tidy data frames along the lines of the 'broom' package. The package provides three S3 generics for each model: tidy(), which summarizes a model's statistical findings such as coefficients of a regression; augment(), which adds columns to the original data such as predictions, residuals and cluster assignments; and glance(), which provides a one-row summary of model-level statistics.},
	urldate = {2022-10-17},
	author = {Bolker, Ben and Robinson, David and Menne, Dieter and Gabry, Jonah and Buerkner, Paul and Hua, Christopher and Petry, William and Wiley, Joshua and Kennedy, Patrick and Szöcs, Szöcs and Patil, Indrajeet and Arel-Bundock, Vincent and Denney, Bill and Brunson, Cory},
	month = apr,
	year = {2022},
}

@misc{peterson_bestnormalize_2022,
	title = {{bestNormalize}: {Normalizing} {Transformation} {Functions}},
	copyright = {GPL-3},
	shorttitle = {{bestNormalize}},
	url = {https://CRAN.R-project.org/package=bestNormalize},
	abstract = {Estimate a suite of normalizing transformations, including a new adaptation of a technique based on ranks which can guarantee normally distributed transformed data if there are no ties: ordered quantile normalization (ORQ). ORQ normalization combines a rank-mapping approach with a shifted logit approximation that allows the transformation to work on data outside the original domain. It is also able to handle new data within the original domain via linear interpolation. The package is built to estimate the best normalizing transformation for a vector consistently and accurately. It implements the Box-Cox transformation, the Yeo-Johnson transformation, three types of Lambert WxF transformations, and the ordered quantile normalization transformation. It estimates the normalization efficacy of other commonly used transformations, and it allows users to specify custom transformations or normalization statistics. Finally, functionality can be integrated into a machine learning workflow via recipes.},
	urldate = {2022-10-17},
	author = {Peterson, Ryan Andrew},
	month = jun,
	year = {2022},
}

@misc{wilke_ggridges_2022,
	title = {ggridges: {Ridgeline} {Plots} in 'ggplot2'},
	copyright = {GPL-2 {\textbar} file LICENSE},
	shorttitle = {ggridges},
	url = {https://CRAN.R-project.org/package=ggridges},
	abstract = {Ridgeline plots provide a convenient way of visualizing changes in distributions over time or space. This package enables the creation of such plots in 'ggplot2'.},
	urldate = {2022-10-17},
	author = {Wilke, Claus O.},
	month = sep,
	year = {2022},
}

@misc{wickham_ggplot2_2022,
	title = {ggplot2: {Create} {Elegant} {Data} {Visualisations} {Using} the {Grammar} of {Graphics}},
	copyright = {MIT + file LICENSE},
	shorttitle = {ggplot2},
	url = {https://CRAN.R-project.org/package=ggplot2},
	abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
	month = may,
	year = {2022},
	keywords = {TeachingStatistics, Spatial},
}

@misc{hester_glue_2022,
	title = {glue: {Interpreted} {String} {Literals}},
	copyright = {MIT + file LICENSE},
	shorttitle = {glue},
	url = {https://CRAN.R-project.org/package=glue},
	abstract = {An implementation of interpreted string literals, inspired by Python's Literal String Interpolation {\textless}https://www.python.org/dev/peps/pep-0498/{\textgreater} and Docstrings {\textless}https://www.python.org/dev/peps/pep-0257/{\textgreater} and Julia's Triple-Quoted String Literals {\textless}https://docs.julialang.org/en/v1.3/manual/strings/\#Triple-Quoted-String-Literals-1{\textgreater}.},
	urldate = {2022-10-17},
	author = {Hester, Jim and Bryan, Jennifer},
	month = feb,
	year = {2022},
}

@misc{wilke_cowplot_2020,
	title = {cowplot: {Streamlined} {Plot} {Theme} and {Plot} {Annotations} for 'ggplot2'},
	copyright = {GPL-2},
	shorttitle = {cowplot},
	url = {https://CRAN.R-project.org/package=cowplot},
	abstract = {Provides various features that help with creating publication-quality figures with 'ggplot2', such as a set of themes, functions to align plots and arrange them into complex compound figures, and functions that make it easy to annotate plots and or mix plots with images. The package was originally written for internal use in the Wilke lab, hence the name (Claus O. Wilke's plot package). It has also been used extensively in the book Fundamentals of Data Visualization.},
	urldate = {2022-10-17},
	author = {Wilke, Claus O.},
	month = dec,
	year = {2020},
}

@misc{kay_ggdist_2022,
	title = {ggdist: {Visualizations} of {Distributions} and {Uncertainty}},
	copyright = {GPL (≥ 3)},
	shorttitle = {ggdist},
	url = {https://CRAN.R-project.org/package=ggdist},
	abstract = {Provides primitives for visualizing distributions using 'ggplot2' that are particularly tuned for visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as bootstrap distributions or Bayesian posterior samples) are easily visualized. Visualization primitives include but are not limited to: points with multiple uncertainty intervals, eye plots (Spiegelhalter D., 1999) {\textless}https://ideas.repec.org/a/bla/jorssa/v162y1999i1p45-58.html{\textgreater}, density plots, gradient plots, dot plots (Wilkinson L., 1999) {\textless}doi:10.1080/00031305.1999.10474474{\textgreater}, quantile dot plots (Kay M., Kola T., Hullman J., Munson S., 2016) {\textless}doi:10.1145/2858036.2858558{\textgreater}, complementary cumulative distribution function barplots (Fernandes M., Walls L., Munson S., Hullman J., Kay M., 2018) {\textless}doi:10.1145/3173574.3173718{\textgreater}, and fit curves with multiple uncertainty ribbons.},
	urldate = {2022-10-17},
	author = {Kay, Matthew and Wiernik, Brenton M.},
	month = jul,
	year = {2022},
}

@misc{tiedemann_gghalves_2022,
	title = {gghalves: {Compose} {Half}-{Half} {Plots} {Using} {Your} {Favourite} {Geoms}},
	copyright = {MIT + file LICENSE},
	shorttitle = {gghalves},
	url = {https://CRAN.R-project.org/package=gghalves},
	abstract = {A 'ggplot2' extension for easy plotting of half-half geom combinations. Think half boxplot and half jitterplot, or half violinplot and half dotplot.},
	urldate = {2022-10-17},
	author = {Tiedemann, Frederik},
	month = may,
	year = {2022},
}

@misc{campitelli_ggnewscale_2022,
	title = {ggnewscale: {Multiple} {Fill} and {Colour} {Scales} in 'ggplot2'},
	copyright = {GPL-3},
	shorttitle = {ggnewscale},
	url = {https://CRAN.R-project.org/package=ggnewscale},
	abstract = {Use multiple fill and colour scales in 'ggplot2'.},
	urldate = {2022-10-17},
	author = {Campitelli, Elio},
	month = oct,
	year = {2022},
}

@misc{ihaka_colorspace_2022,
	title = {colorspace: {A} {Toolbox} for {Manipulating} and {Assessing} {Colors} and {Palettes}},
	copyright = {BSD\_3\_clause + file LICENSE},
	shorttitle = {colorspace},
	url = {https://CRAN.R-project.org/package=colorspace},
	abstract = {Carries out mapping between assorted color spaces including RGB, HSV, HLS, CIEXYZ, CIELUV, HCL (polar CIELUV), CIELAB, and polar CIELAB. Qualitative, sequential, and diverging color palettes based on HCL colors are provided along with corresponding ggplot2 color scales. Color palette choice is aided by an interactive app (with either a Tcl/Tk or a shiny graphical user interface) and shiny apps with an HCL color picker and a color vision deficiency emulator. Plotting functions for displaying and assessing palettes include color swatches, visualizations of the HCL space, and trajectories in HCL and/or RGB spectrum. Color manipulation functions include: desaturation, lightening/darkening, mixing, and simulation of color vision deficiencies (deutanomaly, protanomaly, tritanomaly). Details can be found on the project web page at {\textless}https://colorspace.R-Forge.R-project.org/{\textgreater} and in the accompanying scientific paper: Zeileis et al. (2020, Journal of Statistical Software, {\textless}doi:10.18637/jss.v096.i01{\textgreater}).},
	urldate = {2022-10-17},
	author = {Ihaka, Ross and Murrell, Paul and Hornik, Kurt and Fisher, Jason C. and Stauffer, Reto and Wilke, Claus O. and McWhite, Claire D. and Zeileis, Achim},
	month = feb,
	year = {2022},
}

@misc{wilke_ggtext_2022,
	title = {ggtext: {Improved} {Text} {Rendering} {Support} for 'ggplot2'},
	copyright = {GPL-2},
	shorttitle = {ggtext},
	url = {https://CRAN.R-project.org/package=ggtext},
	abstract = {A 'ggplot2' extension that enables the rendering of complex formatted plot labels (titles, subtitles, facet labels, axis labels, etc.). Text boxes with automatic word wrap are also supported.},
	urldate = {2022-10-17},
	author = {Wilke, Claus O. and Wiernik, Brenton M.},
	month = sep,
	year = {2022},
}

@misc{wilke_ungewiz_2022,
	title = {Ungewiz: {Tools} for visualizing uncertainty with ggplot2},
	url = {https://github.com/wilkelab/ungeviz},
	author = {Wilke, Claus},
	year = {2022},
}

@misc{pedersen_scico_2022,
	title = {scico: {Colour} {Palettes} {Based} on the {Scientific} {Colour}-{Maps}},
	copyright = {MIT + file LICENSE},
	shorttitle = {scico},
	url = {https://CRAN.R-project.org/package=scico},
	abstract = {Colour choice in information visualisation is important in order to avoid being mislead by inherent bias in the used colour palette. The 'scico' package provides access to the perceptually uniform and colour-blindness friendly palettes developed by Fabio Crameri and released under the "Scientific Colour-Maps" moniker. The package contains 24 different palettes and includes both diverging and sequential types.},
	urldate = {2022-10-17},
	author = {Pedersen, Thomas Lin and Crameri, Fabio},
	month = aug,
	year = {2022},
}

@article{chambers_random_2013,
	title = {A {Random} {Effect} {Block} {Bootstrap} for {Clustered} {Data}},
	volume = {22},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2012.681216},
	doi = {10.1080/10618600.2012.681216},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Chambers, Raymond and Chandra, Hukum},
	month = apr,
	year = {2013},
	note = {Publisher: Taylor \& Francis},
	keywords = {notion},
	pages = {452--470},
	annote = {doi: 10.1080/10618600.2012.681216},
}

@article{meteyard_best_2020,
	title = {Best practice guidance for linear mixed-effects models in psychological science},
	volume = {112},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300061},
	doi = {10.1016/j.jml.2020.104092},
	abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods – a survey of researchers (n = 163) and a quasi-systematic review of papers using LMMs (n = 400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors’ intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
	journal = {Journal of Memory and Language},
	author = {Meteyard, Lotte and Davies, Robert A.I.},
	month = jun,
	year = {2020},
	keywords = {Hierarchical models, Linear mixed effects models, Multilevel models, notion},
	pages = {104092},
}

@article{meteyard_best_2020-1,
	title = {Best practice guidance for linear mixed-effects models in psychological science},
	volume = {112},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300061},
	doi = {10.1016/j.jml.2020.104092},
	abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods – a survey of researchers (n = 163) and a quasi-systematic review of papers using LMMs (n = 400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors’ intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
	language = {en},
	urldate = {2022-10-18},
	journal = {Journal of Memory and Language},
	author = {Meteyard, Lotte and Davies, Robert A. I.},
	month = jun,
	year = {2020},
	keywords = {Hierarchical models, Linear mixed effects models, Multilevel models, notion},
	pages = {104092},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/UKV7HSXJ/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf:application/pdf;ScienceDirect Snapshot:/Users/luca/Zotero/storage/WWBLG2BW/S0749596X20300061.html:text/html},
}

@article{schad_how_2020,
	title = {How to capitalize on a priori contrasts in linear (mixed) models: {A} tutorial},
	volume = {110},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X19300695},
	doi = {10.1016/j.jml.2019.104038},
	abstract = {Factorial experiments in research on memory, language, and in other areas are often analyzed using analysis of variance (ANOVA). However, for effects with more than one numerator degrees of freedom, e.g., for experimental factors with more than two levels, the ANOVA omnibus F-test is not informative about the source of a main effect or interaction. Because researchers typically have specific hypotheses about which condition means differ from each other, a priori contrasts (i.e., comparisons planned before the sample means are known) between specific conditions or combinations of conditions are the appropriate way to represent such hypotheses in the statistical model. Many researchers have pointed out that contrasts should be “tested instead of, rather than as a supplement to, the ordinary ‘omnibus’ F test” (Hays, 1973, p. 601). In this tutorial, we explain the mathematics underlying different kinds of contrasts (i.e., treatment, sum, repeated, polynomial, custom, nested, interaction contrasts), discuss their properties, and demonstrate how they are applied in the R System for Statistical Computing (R Core Team, 2018). In this context, we explain the generalized inverse which is needed to compute the coefficients for contrasts that test hypotheses that are not covered by the default set of contrasts. A detailed understanding of contrast coding is crucial for successful and correct specification in linear models (including linear mixed models). Contrasts defined a priori yield far more useful confirmatory tests of experimental hypotheses than standard omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
	journal = {Journal of Memory and Language},
	author = {Schad, Daniel J. and Vasishth, Shravan and Hohenstein, Sven and Kliegl, Reinhold},
	month = feb,
	year = {2020},
	keywords = {A priori hypotheses, Contrasts, Linear models, Null hypothesis significance testing, notion},
	pages = {104038},
}

@article{mohr_boisberlin_2016,
	title = {{BOiS}—{Berlin} {Object} in {Scene} {Database}: {Controlled} {Photographic} {Images} for {Visual} {Search} {Experiments} with {Quantified} {Contextual} {Priors}},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.00749},
	journal = {Frontiers in Psychology},
	author = {Mohr, Johannes and Seyfarth, Julia and Lueschow, Andreas and Weber, Joachim E. and Wichmann, Felix A. and Obermayer, Klaus},
	year = {2016},
	keywords = {notion},
}

@article{brehm_contrast_2022,
	title = {Contrast coding choices in a decade of mixed models},
	volume = {125},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X22000213},
	doi = {10.1016/j.jml.2022.104334},
	abstract = {Contrast coding in regression models, including mixed-effect models, changes what the terms in the model mean. In particular, it determines whether or not model terms should be interpreted as main effects. This paper highlights how opaque descriptions of contrast coding have affected the field of psycholinguistics. We begin with a reproducible example in R using simulated data to demonstrate how incorrect conclusions can be made from mixed models; this also serves as a primer on contrast coding for statistical novices. We then present an analysis of 3384 papers from the field of psycholinguistics that we coded based upon whether a clear description of contrast coding was present. This analysis demonstrates that the majority of the psycholinguistic literature does not transparently describe contrast coding choices, posing an important challenge to reproducibility and replicability in our field.},
	journal = {Journal of Memory and Language},
	author = {Brehm, Laurel and Alday, Phillip M.},
	month = aug,
	year = {2022},
	keywords = {Contrasts, notion, Meta-science, Mixed effect models, Replication crisis},
	pages = {104334},
}

@article{brehm_contrast_2022-1,
	title = {Contrast coding choices in a decade of mixed models},
	volume = {125},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X22000213},
	doi = {10.1016/j.jml.2022.104334},
	abstract = {Contrast coding in regression models, including mixed-effect models, changes what the terms in the model mean. In particular, it determines whether or not model terms should be interpreted as main effects. This paper highlights how opaque descriptions of contrast coding have affected the field of psycholinguistics. We begin with a reproducible example in R using simulated data to demonstrate how incorrect conclusions can be made from mixed models; this also serves as a primer on contrast coding for statistical novices. We then present an analysis of 3384 papers from the field of psycholinguistics that we coded based upon whether a clear description of contrast coding was present. This analysis demonstrates that the majority of the psycholinguistic literature does not transparently describe contrast coding choices, posing an important challenge to reproducibility and replicability in our field.},
	language = {en},
	urldate = {2022-10-20},
	journal = {Journal of Memory and Language},
	author = {Brehm, Laurel and Alday, Phillip M.},
	month = aug,
	year = {2022},
	keywords = {Contrasts, notion, Meta-science, Mixed effect models, Replication crisis},
	pages = {104334},
	file = {ScienceDirect Snapshot:/Users/luca/Zotero/storage/IZXSAIVI/S0749596X22000213.html:text/html},
}

@article{harrison_brief_2018,
	title = {A brief introduction to mixed effects modelling and multi-model inference in ecology},
	volume = {6},
	doi = {10.7717/peerj.4794},
	journal = {PeerJ},
	author = {Harrison, Xavier and Donaldson, Lynda and Correa, Maru and Evans, Julian and Fisher, David and Goodwin, Cecily and Robinson, Beth and Hodgson, David and Inger, Richard},
	month = may,
	year = {2018},
	pages = {e4794},
}

@article{meteyard_best_2020-2,
	title = {Best practice guidance for linear mixed-effects models in psychological science},
	volume = {112},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300061},
	doi = {10.1016/j.jml.2020.104092},
	abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods – a survey of researchers (n = 163) and a quasi-systematic review of papers using LMMs (n = 400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors’ intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
	language = {en},
	urldate = {2022-10-21},
	journal = {Journal of Memory and Language},
	author = {Meteyard, Lotte and Davies, Robert A. I.},
	month = jun,
	year = {2020},
	keywords = {Hierarchical models, Linear mixed effects models, Multilevel models},
	pages = {104092},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/SP6E8VQY/Meteyard und Davies - 2020 - Best practice guidance for linear mixed-effects mo.pdf:application/pdf;ScienceDirect Snapshot:/Users/luca/Zotero/storage/4C9QFJCN/S0749596X20300061.html:text/html},
}

@incollection{singmann_introduction_2019-1,
	title = {An introduction to mixed models for experimental psychology},
	booktitle = {New methods in cognitive psychology},
	publisher = {Routledge},
	author = {Singmann, H and Kellen, David},
	year = {2019},
	pages = {4--31},
}

@article{bono_report_2021,
	title = {Report {Quality} of {Generalized} {Linear} {Mixed} {Models} in {Psychology}: {A} {Systematic} {Review}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {Report {Quality} of {Generalized} {Linear} {Mixed} {Models} in {Psychology}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.666182},
	abstract = {Generalized linear mixed models (GLMMs) estimate fixed and random effects and are especially useful when the dependent variable is binary, ordinal, count or quantitative but not normally distributed. They are also useful when the dependent variable involves repeated measures, since GLMMs can model autocorrelation. This study aimed to determine how and how often GLMMs are used in psychology and to summarize how the information about them is presented in published articles. Our focus in this respect was mainly on frequentist models. In order to review studies applying GLMMs in psychology we searched the Web of Science for articles published over the period 2014–2018. A total of 316 empirical articles were selected for trend study from 2014 to 2018. We then conducted a systematic review of 118 GLMM analyses from 80 empirical articles indexed in Journal Citation Reports during 2018 in order to evaluate report quality. Results showed that the use of GLMMs increased over time and that 86.4\% of articles were published in first- or second-quartile journals. Although GLMMs have, in recent years, been increasingly used in psychology, most of the important information about them was not stated in the majority of articles. Report quality needs to be improved in line with current recommendations for the use of GLMMs.},
	urldate = {2022-10-21},
	journal = {Frontiers in Psychology},
	author = {Bono, Roser and Alarcón, Rafael and Blanca, María J.},
	year = {2021},
	file = {Full Text PDF:/Users/luca/Zotero/storage/S2PDVGJM/Bono et al. - 2021 - Report Quality of Generalized Linear Mixed Models .pdf:application/pdf},
}

@article{blanca_current_2018,
	title = {Current {Practices} in {Data} {Analysis} {Procedures} in {Psychology}: {What} {Has} {Changed}?},
	volume = {9},
	issn = {1664-1078},
	shorttitle = {Current {Practices} in {Data} {Analysis} {Procedures} in {Psychology}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02558},
	abstract = {This paper analyzes current practices in psychology in the use of research methods and data analysis procedures (DAP) and aims to determine whether researchers are now using more sophisticated and advanced DAP than were employed previously. We reviewed empirical research published recently in prominent journals from the USA and Europe corresponding to the main psychological categories of Journal Citation Reports and examined research methods, number of studies, number and type of DAP, and statistical package. The 288 papers reviewed used 663 different DAP. Experimental and correlational studies were the most prevalent, depending on the specific field of psychology. Two-thirds of the papers reported a single study, although those in journals with an experimental focus typically described more. The papers mainly used parametric tests for comparison and statistical techniques for analyzing relationships among variables. Regarding the former, the most frequently used procedure was ANOVA, with mixed factorial ANOVA being the most prevalent. A decline in the use of non-parametric analysis was observed in relation to previous research. Relationships among variables were most commonly examined using regression models, with hierarchical regression and mediation analysis being the most prevalent procedures. There was also a decline in the use of stepwise regression and an increase in the use of structural equation modeling, confirmatory factor analysis, and hierarchical linear modeling. Overall, the results show that recent empirical studies published in journals belonging to the main areas of psychology are employing more varied and advanced statistical techniques of greater computational complexity.},
	urldate = {2022-10-21},
	journal = {Frontiers in Psychology},
	author = {Blanca, María J. and Alarcón, Rafael and Bono, Roser},
	year = {2018},
	file = {Full Text PDF:/Users/luca/Zotero/storage/AER4XKDS/Blanca et al. - 2018 - Current Practices in Data Analysis Procedures in P.pdf:application/pdf},
}

@incollection{van_zandt_analysis_2002,
	title = {Analysis of response time distributions},
	volume = {4},
	booktitle = {Stevens’ handbook of experimental psychology},
	author = {Van Zandt, T},
	year = {2002},
	pages = {461--516},
}

@article{micceri_unicorn_1989,
	title = {The unicorn, the normal curve, and other improbable creatures},
	volume = {105},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.105.1.156},
	abstract = {An investigation of the distributional characteristics of 440 large-sample achievement and psychometric measures found all to be significantly nonnormal at the alpha .01 significance level. Several classes of contamination were found, including tail weights from the uniform to the double exponential, exponential-level asymmetry, severe digit preferences, multimodalities, and modes external to the mean/median interval. Thus, the underlying tenets of normality-assuming statistics appear fallacious for these commonly used types of data. However, findings here also fail to support the types of distributions used in most prior robustness research suggesting the failure of such statistics under nonnormal conditions. A reevaluation of the statistical robustness literature appears appropriate in light of these findings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Psychological Bulletin},
	author = {Micceri, Theodore},
	year = {1989},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Psychometrics, Achievement Measures, Normal Distribution, Statistical Measurement},
	pages = {156--166},
	file = {Snapshot:/Users/luca/Zotero/storage/ZZIQ9I7F/1989-14214-001.html:text/html},
}

@article{blanca_skewness_2013,
	title = {Skewness and kurtosis in real data samples},
	volume = {9},
	issn = {1614-2241},
	doi = {10.1027/1614-2241/a000057},
	abstract = {Parametric statistics are based on the assumption of normality. Recent findings suggest that Type I error and power can be adversely affected when data are non-normal. This paper aims to assess the distributional shape of real data by examining the values of the third and fourth central moments as a measurement of skewness and kurtosis in small samples. The analysis concerned 693 distributions with a sample size ranging from 10 to 30. Measures of cognitive ability and of other psychological variables were included. The results showed that skewness ranged between −2.49 and 2.33. The values of kurtosis ranged between −1.92 and 7.41. Considering skewness and kurtosis together the results indicated that only 5.5\% of distributions were close to expected values under normality. Although extreme contamination does not seem to be very frequent, the findings are consistent with previous research suggesting that normality is not the rule with real data. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
	journal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
	author = {Blanca, María J. and Arnau, Jaume and López-Montiel, Dolores and Bono, Roser and Bendayan, Rebecca},
	year = {2013},
	note = {Place: Germany
Publisher: Hogrefe Publishing},
	keywords = {Normal Distribution, Skewed Distribution, Statistical Data, Statistical Samples, Statistics},
	pages = {78--84},
	file = {Akzeptierte Version:/Users/luca/Zotero/storage/632Q97TB/Blanca et al. - 2013 - Skewness and kurtosis in real data samples.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/4NSVV2FI/2013-13782-004.html:text/html},
}

@article{brockmole_contextual_2006,
	title = {Contextual cueing in naturalistic scenes: {Global} and local contexts},
	volume = {32},
	issn = {1939-1285},
	shorttitle = {Contextual cueing in naturalistic scenes},
	doi = {10.1037/0278-7393.32.4.699},
	abstract = {In contextual cueing, the position of a target within a group of distractors is learned over repeated exposure to a display with reference to a few nearby items rather than to the global pattern created by the elements. The authors contrasted the role of global and local contexts for contextual cueing in naturalistic scenes. Experiment 1 showed that learned target positions transfer when local information is altered but not when global information is changed. Experiment 2 showed that scene-target covariation is learned more slowly when local, but not global, information is repeated across trials than when global but not local information is repeated. Thus, in naturalistic scenes, observers are biased to associate target locations with global contexts. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Brockmole, James R. and Castelhano, Monica S. and Henderson, John M.},
	year = {2006},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Attention, notion, Contextual Associations, Contextual Cues, Cues, Visual Memory},
	pages = {699--706},
	file = {Snapshot:/Users/luca/Zotero/storage/M6XDPVC8/2006-08497-004.html:text/html},
}

@article{jiang_what_2004,
	title = {What is learned in spatial contextual cuing— configuration or individual locations?},
	volume = {66},
	issn = {1532-5962},
	url = {https://doi.org/10.3758/BF03194893},
	doi = {10.3758/BF03194893},
	abstract = {With the use of spatial contextual cuing, we tested whether subjects learned to associate target locations with overall configurations of distractors or with individual locations of distractors. In Experiment 1, subjects were trained on 36 visual search displays that contained 36 sets of distractor locations and 18 target locations. Each target location was paired with two sets of distractor locations on separate trials. After training, the subjects showed perfect transfer to recombined displays, which were created by combining half of one trained distractor set with half of another trained distractor set. This result suggests that individual distractor locations were sufficient to cue the target location. In Experiment 2, the subjects showed good transfer from trained displays to rescaled, displaced, and perceptually regrouped displays, suggesting that the relative locations among items were also learned. Thus, both individual target-distractor associations and configural associations are learned in contextual cuing.},
	language = {en},
	number = {3},
	urldate = {2022-10-23},
	journal = {Perception \& Psychophysics},
	author = {Jiang, Yuhong and Wagner, Laura C.},
	month = apr,
	year = {2004},
	keywords = {notion, Distractor Location, Global Configuration, Implicit Learning, Target Location, Transfer Session},
	pages = {454--463},
	file = {Full Text PDF:/Users/luca/Zotero/storage/SPBNIRTE/Jiang und Wagner - 2004 - What is learned in spatial contextual cuing— confi.pdf:application/pdf},
}

@article{lo_transform_2015,
	title = {To transform or not to transform: using generalized linear mixed models to analyse reaction time data},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {To transform or not to transform},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171},
	abstract = {Linear mixed-effect models (LMMs) are being increasingly widely used in psychology to analyse multi-level research designs. This feature allows LMMs to address some of the problems identified by Speelman and McGann (2013) about the use of mean data, because they do not average across individual responses. However, recent guidelines for using LMM to analyse skewed reaction time (RT) data collected in many cognitive psychological studies recommend the application of non-linear transformations to satisfy assumptions of normality. Uncritical adoption of this recommendation has important theoretical implications which can yield misleading conclusions. For example, Balota et al. (2013) showed that analyses of raw RT produced additive effects of word frequency and stimulus quality on word identification, which conflicted with the interactive effects observed in analyses of transformed RT. Generalized linear mixed-effect models (GLMM) provide a solution to this problem by satisfying normality assumptions without the need for transformation. This allows differences between individuals to be properly assessed, using the metric most appropriate to the researcher's theoretical context. We outline the major theoretical decisions involved in specifying a GLMM, and illustrate them by reanalysing Balota et al.'s datasets. We then consider the broader benefits of using GLMM to investigate individual differences.},
	urldate = {2022-10-23},
	journal = {Frontiers in Psychology},
	author = {Lo, Steson and Andrews, Sally},
	year = {2015},
	keywords = {notion},
	file = {Full Text PDF:/Users/luca/Zotero/storage/6HYKQWE6/Lo und Andrews - 2015 - To transform or not to transform using generalize.pdf:application/pdf},
}

@article{box_analysis_1964,
	title = {An analysis of transformations},
	volume = {26},
	journal = {Journal of the Royal Statistical Society B},
	author = {Box, G. E. P. and Cox, D. R.},
	year = {1964},
	pages = {211--252},
}

@article{yeo_new_2000,
	title = {A new family of power transformations to improve normality or symmetry},
	volume = {87},
	doi = {10.1093/biomet/87.4.954},
	journal = {Biometrika},
	author = {Yeo, In-Kwon and Johnson, Richard},
	month = dec,
	year = {2000},
}

@misc{aust_papaja_2022,
	title = {papaja: {Prepare} {American} {Psychological} {Association} {Journal} {Articles} with {R} {Markdown}},
	copyright = {MIT + file LICENSE},
	shorttitle = {papaja},
	url = {https://CRAN.R-project.org/package=papaja},
	abstract = {Tools to create dynamic, submission-ready manuscripts, which conform to American Psychological Association manuscript guidelines. We provide R Markdown document formats for manuscripts (PDF and Word) and revision letters (PDF). Helper functions facilitate reporting statistical analyses or create publication-ready tables and plots.},
	urldate = {2022-10-25},
	author = {Aust, Frederik and Barth, Marius and Diedenhofen, Birk and Stahl, Christoph and Casillas, Joseph V. and Siegel, Rudolf},
	month = jul,
	year = {2022},
}

@misc{xie_knitr_2022,
	title = {knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in {R}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL]},
	shorttitle = {knitr},
	url = {https://CRAN.R-project.org/package=knitr},
	abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
	urldate = {2022-10-25},
	author = {Xie, Yihui and Sarma, Abhraneel and Vogt, Adam and Andrew, Alastair and Zvoleff, Alex and Al-Zubaidi, Amar and Simon, Andre and Atkins, Aron and Wolen, Aaron and Manton, Ashley and Yasumoto, Atsushi and Baumer, Ben and Diggs, Brian and Zhang, Brian and Yapparov, Bulat and Pereira, Cassio and Dervieux, Christophe and Hall, David and Hugh-Jones, David and Robinson, David and Hemken, Doug and Murdoch, Duncan and Campitelli, Elio and Hughes, Ellis and Riederer, Emily and Hirschmann, Fabian and Simeon, Fitch and Fang, Forest and Harrell Jr, Frank E. and Aden-Buie, Garrick and Detrez, Gregoire and Wickham, Hadley and Zhu, Hao and Jeon, Heewon and Bengtsson, Henrik and Yutani, Hiroaki and Lyttle, Ian and Daniel, Hodges and Bien, Jacob and Burkhead, Jake and Manton, James and Lander, Jared and Punyon, Jason and Luraschi, Javier and Arnold, Jeff and Bryan, Jenny and Ashkenas, Jeremy and Stephens, Jeremy and Hester, Jim and Cheng, Joe and Ranke, Johannes and Honaker, John and Muschelli, John and Keane, Jonathan and Allaire, J. J. and Toloe, Johan and Sidi, Jonathan and Larmarange, Joseph and Barnier, Julien and Zhong, Kaiyin and Slowikowski, Kamil and Forner, Karl and Smith, Kevin K. and Mueller, Kirill and Takahashi, Kohske and Walthert, Lorenz and Gallindo, Lucas and Hofert, Marius and Modrák, Martin and Chirico, Michael and Friendly, Michael and Bojanowski, Michal and Kuhlmann, Michel and Patrick, Miller and Caballero, Nacho and Salkowski, Nick and Hansen, Niels Richard and Ross, Noam and Mahdi, Obada and Krivitsky, Pavel N. and Li, Qiang and Vaidyanathan, Ramnath and Cotton, Richard and Krzyzanowski, Robert and Copetti, Rodrigo and Francois, Romain and Williamson, Ruaridh and Mati, Sagiru and Kostyshak, Scott and Meyer, Sebastian and Brouwer, Sietse and Bernard, Simon de and Rousseau, Sylvain and Wei, Taiyun and Assus, Thibaut and Lamadon, Thibaut and Leeper, Thomas and Mastny, Tim and Torsney-Weir, Tom and Davis, Trevor and Veitas, Viktoras and Zhu, Weicheng and Wu, Wush and Foster, Zachary and Kamvar, Zhian N.},
	month = aug,
	year = {2022},
	keywords = {ReproducibleResearch},
}

@misc{zhu_kableextra_2021,
	title = {{kableExtra}: {Construct} {Complex} {Table} with 'kable' and {Pipe} {Syntax}},
	copyright = {MIT + file LICENSE},
	shorttitle = {{kableExtra}},
	url = {https://CRAN.R-project.org/package=kableExtra},
	abstract = {Build complex HTML or 'LaTeX' tables using 'kable()' from 'knitr' and the piping syntax from 'magrittr'. Function 'kable()' is a light weight table generator coming from 'knitr'. This package simplifies the way to manipulate the HTML or 'LaTeX' codes generated by 'kable()' and allows users to construct complex tables and customize styles using a readable syntax.},
	urldate = {2022-10-25},
	author = {Zhu, Hao and Travison, Thomas and Tsai, Timothy and Beasley, Will and Xie, Yihui and Yu, GuangChuang and Laurent, Stéphane and Shepherd, Rob and Sidi, Yoni and Salzer, Brian and Gui, George and Fan, Yeliang and Murdoch, Duncan and Evans, Bill},
	month = feb,
	year = {2021},
}

@article{goujon_categorical_2011,
	title = {Categorical implicit learning in real-world scenes: {Evidence} from contextual cueing},
	volume = {64},
	issn = {1747-0218},
	shorttitle = {Categorical implicit learning in real-world scenes},
	url = {https://www.tandfonline.com/doi/abs/10.1080/17470218.2010.526231},
	doi = {10.1080/17470218.2010.526231},
	abstract = {The present study examined the extent to which learning mechanisms are deployed on semantic-categorical regularities during a visual searching within real-world scenes. The contextual cueing paradigm was used with photographs of indoor scenes in which the semantic category did or did not predict the target position on the screen. No evidence of a facilitation effect was observed in the predictive condition compared to the nonpredictive condition when participants were merely instructed to search for a target T or L (Experiment 1). However, a rapid contextual cueing effect occurred when each display containing the search target was preceded by a preview of the scene on which participants had to make a decision regarding the scene's category (Experiment 2). A follow-up explicit memory task indicated that this benefit resulted from implicit learning. Similar implicit contextual cueing effects were also obtained when the scene to categorize was different from the subsequent search scene (Experiment 3) and when a mere preview of the search scene preceded the visual searching (Experiment 4). These results suggested that if enhancing the processing of the scene was required with the present material, such implicit semantic learning can nevertheless take place when the category is task irrelevant.},
	number = {5},
	urldate = {2022-10-26},
	journal = {The Quarterly Journal of Experimental Psychology},
	author = {Goujon, Annabelle},
	month = may,
	year = {2011},
	note = {Publisher: Routledge
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/17470218.2010.526231},
	keywords = {Implicit learning, notion, Categorization, Contextual cueing, Real-world scenes, Semantic regularities, Visual search},
	pages = {920--941},
}

@misc{noauthor_interaction_nodate,
	title = {Interaction between scene-based and array-based contextual cueing {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.3758/s13414-013-0446-9},
	urldate = {2022-10-26},
	keywords = {notion},
	file = {Interaction between scene-based and array-based contextual cueing | SpringerLink:/Users/luca/Zotero/storage/HMQ5M8JJ/s13414-013-0446-9.html:text/html},
}

@article{rosenbaum_interaction_2013,
	title = {Interaction between scene-based and array-based contextual cueing},
	volume = {75},
	issn = {1943-393X},
	url = {https://doi.org/10.3758/s13414-013-0446-9},
	doi = {10.3758/s13414-013-0446-9},
	abstract = {Contextual cueing refers to the cueing of spatial attention by repeated spatial context. Previous studies have demonstrated distinctive properties of contextual cueing by background scenes and by an array of search items. Whereas scene-based contextual cueing reflects explicit learning of the scene–target association, array-based contextual cueing is supported primarily by implicit learning. In this study, we investigated the interaction between scene-based and array-based contextual cueing. Participants searched for a target that was predicted by both the background scene and the locations of distractor items. We tested three possible patterns of interaction: (1) The scene and the array could be learned independently, in which case cueing should be expressed even when only one cue was preserved; (2) the scene and array could be learned jointly, in which case cueing should occur only when both cues were preserved; (3) overshadowing might occur, in which case learning of the stronger cue should preclude learning of the weaker cue. In several experiments, we manipulated the nature of the contextual cues present during training and testing. We also tested explicit awareness of scenes, scene–target associations, and arrays. The results supported the overshadowing account: Specifically, scene-based contextual cueing precluded array-based contextual cueing when both were predictive of the location of a search target. We suggest that explicit, endogenous cues dominate over implicit cues in guiding spatial attention.},
	language = {en},
	number = {5},
	urldate = {2022-10-26},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Rosenbaum, Gail M. and Jiang, Yuhong V.},
	month = jul,
	year = {2013},
	keywords = {Attention, Contextual cueing, Visual search, Implicit/explicit learning},
	pages = {888--899},
	file = {Full Text PDF:/Users/luca/Zotero/storage/D4ECCWYZ/Rosenbaum und Jiang - 2013 - Interaction between scene-based and array-based co.pdf:application/pdf},
}

@article{goujon_investigating_2015,
	title = {Investigating implicit statistical learning mechanisms through contextual cueing},
	volume = {19},
	issn = {1879-307X},
	doi = {10.1016/j.tics.2015.07.009},
	abstract = {Since its inception, the contextual cueing (CC) paradigm has generated considerable interest in various fields of cognitive sciences because it constitutes an elegant approach to understanding how statistical learning (SL) mechanisms can detect contextual regularities during a visual search. In this article we review and discuss five aspects of CC: (i) the implicit nature of learning, (ii) the mechanisms involved in CC, (iii) the mediating factors affecting CC, (iv) the generalization of CC phenomena, and (v) the dissociation between implicit and explicit CC phenomena. The findings suggest that implicit SL is an inherent component of ongoing processing which operates through clustering, associative, and reinforcement processes at various levels of sensory-motor processing, and might result from simple spike-timing-dependent plasticity.},
	language = {eng},
	number = {9},
	journal = {Trends in Cognitive Sciences},
	author = {Goujon, Annabelle and Didierjean, André and Thorpe, Simon},
	month = sep,
	year = {2015},
	pmid = {26255970},
	keywords = {Humans, Cognition, Brain, Learning, Attention, notion, Cues, Brain Mapping, Photic Stimulation},
	pages = {524--533},
}

@article{conci_contextual_2011,
	title = {Contextual remapping in visual search after predictable target-location changes},
	volume = {75},
	issn = {1430-2772},
	url = {https://doi.org/10.1007/s00426-010-0306-3},
	doi = {10.1007/s00426-010-0306-3},
	abstract = {Invariant spatial context can facilitate visual search. For instance, detection of a target is faster if it is presented within a repeatedly encountered, as compared to a novel, layout of nontargets, demonstrating a role of contextual learning for attentional guidance (‘contextual cueing’). Here, we investigated how context-based learning adapts to target location (and identity) changes. Three experiments were performed in which, in an initial learning phase, observers learned to associate a given context with a given target location. A subsequent test phase then introduced identity and/or location changes to the target. The results showed that contextual cueing could not compensate for target changes that were not ‘predictable’ (i.e. learnable). However, for predictable changes, contextual cueing remained effective even immediately after the change. These findings demonstrate that contextual cueing is adaptive to predictable target location changes. Under these conditions, learned contextual associations can be effectively ‘remapped’ to accommodate new task requirements.},
	language = {en},
	number = {4},
	urldate = {2022-10-26},
	journal = {Psychological Research},
	author = {Conci, Markus and Sun, Luning and Müller, Hermann J.},
	month = jul,
	year = {2011},
	keywords = {Target Location, Proactive Interference, Recognition Test, Search Display, Visual Search},
	pages = {279--289},
	file = {Full Text PDF:/Users/luca/Zotero/storage/JU68EAYP/Conci et al. - 2011 - Contextual remapping in visual search after predic.pdf:application/pdf},
}

@article{oliva_role_2007,
	title = {The role of context in object recognition},
	volume = {11},
	issn = {1364-6613},
	doi = {10.1016/j.tics.2007.09.009},
	abstract = {In the real world, objects never occur in isolation; they co-vary with other objects and particular environments, providing a rich source of contextual associations to be exploited by the visual system. A natural way of representing the context of an object is in terms of its relationship to other objects. Alternately, recent work has shown that a statistical summary of the scene provides a complementary and effective source of information for contextual inference, which enables humans to quickly guide their attention and eyes to regions of interest in natural scenes. A better understanding of how humans build such scene representations, and of the mechanisms of contextual analysis, will lead to a new generation of computer vision systems. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Trends in Cognitive Sciences},
	author = {Oliva, Aude and Torralba, Antonio},
	year = {2007},
	note = {Place: Netherlands
Publisher: Elsevier Science},
	keywords = {notion, Contextual Associations, Implicit Learning, Computers, Inference, Object Recognition, Visual Displays},
	pages = {520--527},
	file = {Snapshot:/Users/luca/Zotero/storage/XTVHMU2F/2007-18755-004.html:text/html},
}

@book{peirce_building_2022,
	edition = {2},
	title = {Building {Experiments} in {PsychoPy}},
	publisher = {Sage},
	author = {Peirce, J. W. and Hirst, R. J. and MacAskill, M. R.},
	year = {2022},
	keywords = {notion},
}

@article{rousselet_percentile_2021,
	title = {The {Percentile} {Bootstrap}: {A} {Primer} {With} {Step}-by-{Step} {Instructions} in {R}},
	volume = {4},
	issn = {2515-2459},
	shorttitle = {The {Percentile} {Bootstrap}},
	url = {https://doi.org/10.1177/2515245920911881},
	doi = {10.1177/2515245920911881},
	abstract = {The percentile bootstrap is the Swiss Army knife of statistics: It is a nonparametric method based on data-driven simulations. It can be applied to many statistical problems, as a substitute to standard parametric approaches, or in situations for which parametric methods do not exist. In this Tutorial, we cover R code to implement the percentile bootstrap to make inferences about central tendency (e.g., means and trimmed means) and spread in a one-sample example and in an example comparing two independent groups. For each example, we explain how to derive a bootstrap distribution and how to get a confidence interval and a p value from that distribution. We also demonstrate how to run a simulation to assess the behavior of the bootstrap. For some purposes, such as making inferences about the mean, the bootstrap performs poorly. But for other purposes, it is the only known method that works well over a broad range of situations. More broadly, combining the percentile bootstrap with robust estimators (i.e., estimators that are not overly sensitive to outliers) can help users gain a deeper understanding of their data than they would using conventional methods.},
	language = {en},
	number = {1},
	urldate = {2022-10-27},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Rousselet, Guillaume A. and Pernet, Cyril R. and Wilcox, Rand R.},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {2515245920911881},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/9BL6ILXY/Rousselet et al. - 2021 - The Percentile Bootstrap A Primer With Step-by-St.pdf:application/pdf},
}

@incollection{leeden_resampling_2008,
	address = {New York, NY},
	title = {Resampling {Multilevel} {Models}},
	isbn = {978-0-387-73186-5},
	url = {https://doi.org/10.1007/978-0-387-73186-5_11},
	booktitle = {Handbook of {Multilevel} {Analysis}},
	publisher = {Springer New York},
	author = {Leeden, Rien van der and Meijer, Erik and Busing, Frank M.T.A.},
	editor = {Leeuw, Jan de and Meijer, Erik},
	year = {2008},
	doi = {10.1007/978-0-387-73186-5_11},
	pages = {401--433},
}

@article{conci_contextual_2011-1,
	title = {Contextual remapping in visual search after predictable target-location changes},
	volume = {75},
	issn = {1430-2772},
	url = {https://doi.org/10.1007/s00426-010-0306-3},
	doi = {10.1007/s00426-010-0306-3},
	abstract = {Invariant spatial context can facilitate visual search. For instance, detection of a target is faster if it is presented within a repeatedly encountered, as compared to a novel, layout of nontargets, demonstrating a role of contextual learning for attentional guidance (‘contextual cueing’). Here, we investigated how context-based learning adapts to target location (and identity) changes. Three experiments were performed in which, in an initial learning phase, observers learned to associate a given context with a given target location. A subsequent test phase then introduced identity and/or location changes to the target. The results showed that contextual cueing could not compensate for target changes that were not ‘predictable’ (i.e. learnable). However, for predictable changes, contextual cueing remained effective even immediately after the change. These findings demonstrate that contextual cueing is adaptive to predictable target location changes. Under these conditions, learned contextual associations can be effectively ‘remapped’ to accommodate new task requirements.},
	language = {en},
	number = {4},
	urldate = {2022-10-27},
	journal = {Psychological Research},
	author = {Conci, Markus and Sun, Luning and Müller, Hermann J.},
	month = jul,
	year = {2011},
	keywords = {Target Location, Proactive Interference, Recognition Test, Search Display, Visual Search},
	pages = {279--289},
	file = {Full Text PDF:/Users/luca/Zotero/storage/9CL64YLZ/Conci et al. - 2011 - Contextual remapping in visual search after predic.pdf:application/pdf},
}

@article{rosenbaum_interaction_2013-1,
	title = {Interaction between scene-based and array-based contextual cueing},
	volume = {75},
	issn = {1943-393X},
	url = {https://doi.org/10.3758/s13414-013-0446-9},
	doi = {10.3758/s13414-013-0446-9},
	abstract = {Contextual cueing refers to the cueing of spatial attention by repeated spatial context. Previous studies have demonstrated distinctive properties of contextual cueing by background scenes and by an array of search items. Whereas scene-based contextual cueing reflects explicit learning of the scene–target association, array-based contextual cueing is supported primarily by implicit learning. In this study, we investigated the interaction between scene-based and array-based contextual cueing. Participants searched for a target that was predicted by both the background scene and the locations of distractor items. We tested three possible patterns of interaction: (1) The scene and the array could be learned independently, in which case cueing should be expressed even when only one cue was preserved; (2) the scene and array could be learned jointly, in which case cueing should occur only when both cues were preserved; (3) overshadowing might occur, in which case learning of the stronger cue should preclude learning of the weaker cue. In several experiments, we manipulated the nature of the contextual cues present during training and testing. We also tested explicit awareness of scenes, scene–target associations, and arrays. The results supported the overshadowing account: Specifically, scene-based contextual cueing precluded array-based contextual cueing when both were predictive of the location of a search target. We suggest that explicit, endogenous cues dominate over implicit cues in guiding spatial attention.},
	language = {en},
	number = {5},
	urldate = {2022-10-27},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Rosenbaum, Gail M. and Jiang, Yuhong V.},
	month = jul,
	year = {2013},
	keywords = {Attention, Contextual cueing, Visual search, Implicit/explicit learning},
	pages = {888--899},
	file = {Full Text PDF:/Users/luca/Zotero/storage/QLBZVTQ8/Rosenbaum und Jiang - 2013 - Interaction between scene-based and array-based co.pdf:application/pdf},
}

@article{rosenbaum_interaction_2013-2,
	title = {Interaction between scene-based and array-based contextual cueing},
	volume = {75},
	issn = {1943-393X},
	url = {https://doi.org/10.3758/s13414-013-0446-9},
	doi = {10.3758/s13414-013-0446-9},
	abstract = {Contextual cueing refers to the cueing of spatial attention by repeated spatial context. Previous studies have demonstrated distinctive properties of contextual cueing by background scenes and by an array of search items. Whereas scene-based contextual cueing reflects explicit learning of the scene–target association, array-based contextual cueing is supported primarily by implicit learning. In this study, we investigated the interaction between scene-based and array-based contextual cueing. Participants searched for a target that was predicted by both the background scene and the locations of distractor items. We tested three possible patterns of interaction: (1) The scene and the array could be learned independently, in which case cueing should be expressed even when only one cue was preserved; (2) the scene and array could be learned jointly, in which case cueing should occur only when both cues were preserved; (3) overshadowing might occur, in which case learning of the stronger cue should preclude learning of the weaker cue. In several experiments, we manipulated the nature of the contextual cues present during training and testing. We also tested explicit awareness of scenes, scene–target associations, and arrays. The results supported the overshadowing account: Specifically, scene-based contextual cueing precluded array-based contextual cueing when both were predictive of the location of a search target. We suggest that explicit, endogenous cues dominate over implicit cues in guiding spatial attention.},
	number = {5},
	journal = {Attention, Perception, \& Psychophysics},
	author = {Rosenbaum, Gail M. and Jiang, Yuhong V.},
	month = jul,
	year = {2013},
	pages = {888--899},
}

@incollection{jiang_contextual_2020,
	address = {New York, NY},
	title = {Contextual {Cueing}},
	isbn = {978-1-4939-9948-4},
	url = {https://doi.org/10.1007/7657_2019_19},
	abstract = {Contextual cueing refers to the facilitation of visual search by the occasional repetition of a visual context. In standard spatial contextual cueing, a visual search target appears in a consistent location within a repeated array of objects. Search time is faster on repeated displays relative to novel displays, even in participants who do not explicitly recognize the repeated displays. Contextual cueing exemplifies the importance of statistical learning and the resulting memory in guiding attention. Because it involves implicit, relational learning, it has been instrumental in understanding brain functions, cognitive changes across the life span, as well as effects of various neurological, neurodevelopmental, and psychiatric conditions. To stimulate further behavioral and brain research on memory-guided attention and to facilitate comparisons across studies, here we provide a methodological guide on the experimental paradigm of contextual cueing and review key findings. We identify factors that influence the strength of the effect and underscore potential pitfalls in experimental design, data analysis, and interpretation.},
	booktitle = {Spatial {Learning} and {Attention} {Guidance}},
	publisher = {Springer US},
	author = {Jiang, Yuhong V. and Sisk, Caitlin A.},
	editor = {Pollmann, Stefan},
	year = {2020},
	doi = {10.1007/7657_2019_19},
	pages = {59--72},
}

@incollection{jiang_contextual_2020-1,
	address = {New York, NY},
	title = {Contextual {Cueing}},
	isbn = {978-1-4939-9948-4},
	url = {https://doi.org/10.1007/7657_2019_19},
	abstract = {Contextual cueing refers to the facilitation of visual search by the occasional repetition of a visual context. In standard spatial contextual cueing, a visual search target appears in a consistent location within a repeated array of objects. Search time is faster on repeated displays relative to novel displays, even in participants who do not explicitly recognize the repeated displays. Contextual cueing exemplifies the importance of statistical learning and the resulting memory in guiding attention. Because it involves implicit, relational learning, it has been instrumental in understanding brain functions, cognitive changes across the life span, as well as effects of various neurological, neurodevelopmental, and psychiatric conditions. To stimulate further behavioral and brain research on memory-guided attention and to facilitate comparisons across studies, here we provide a methodological guide on the experimental paradigm of contextual cueing and review key findings. We identify factors that influence the strength of the effect and underscore potential pitfalls in experimental design, data analysis, and interpretation.},
	booktitle = {Spatial {Learning} and {Attention} {Guidance}},
	publisher = {Springer US},
	author = {Jiang, Yuhong V. and Sisk, Caitlin A.},
	editor = {Pollmann, Stefan},
	year = {2020},
	doi = {10.1007/7657_2019_19},
	pages = {59--72},
}

@incollection{jiang_contextual_2020-2,
	address = {New York, NY},
	title = {Contextual {Cueing}},
	isbn = {978-1-4939-9948-4},
	url = {https://doi.org/10.1007/7657_2019_19},
	abstract = {Contextual cueing refers to the facilitation of visual search by the occasional repetition of a visual context. In standard spatial contextual cueing, a visual search target appears in a consistent location within a repeated array of objects. Search time is faster on repeated displays relative to novel displays, even in participants who do not explicitly recognize the repeated displays. Contextual cueing exemplifies the importance of statistical learning and the resulting memory in guiding attention. Because it involves implicit, relational learning, it has been instrumental in understanding brain functions, cognitive changes across the life span, as well as effects of various neurological, neurodevelopmental, and psychiatric conditions. To stimulate further behavioral and brain research on memory-guided attention and to facilitate comparisons across studies, here we provide a methodological guide on the experimental paradigm of contextual cueing and review key findings. We identify factors that influence the strength of the effect and underscore potential pitfalls in experimental design, data analysis, and interpretation.},
	booktitle = {Spatial {Learning} and {Attention} {Guidance}},
	publisher = {Springer US},
	author = {Jiang, Yuhong V. and Sisk, Caitlin A.},
	editor = {Pollmann, Stefan},
	year = {2020},
	doi = {10.1007/7657_2019_19},
	pages = {59--72},
}

@article{summerfield_orienting_2006,
	title = {Orienting {Attention} {Based} on {Long}-{Term} {Memory} {Experience}},
	volume = {49},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(06)00074-2},
	doi = {10.1016/j.neuron.2006.01.021},
	language = {English},
	number = {6},
	urldate = {2022-10-27},
	journal = {Neuron},
	author = {Summerfield, Jennifer J. and Lepsien, Jöran and Gitelman, Darren R. and Mesulam, M. Marsel and Nobre, Anna C.},
	month = mar,
	year = {2006},
	pmid = {16543137},
	note = {Publisher: Elsevier},
	keywords = {SYSNEURO},
	pages = {905--916},
	file = {Full Text PDF:/Users/luca/Zotero/storage/GTHTRAIX/Summerfield et al. - 2006 - Orienting Attention Based on Long-Term Memory Expe.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/AXDMMKQC/S0896-6273(06)00074-2.html:text/html},
}

@article{summerfield_orienting_2006-1,
	title = {Orienting {Attention} {Based} on {Long}-{Term} {Memory} {Experience}},
	volume = {49},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(06)00074-2},
	doi = {10.1016/j.neuron.2006.01.021},
	language = {English},
	number = {6},
	urldate = {2022-10-27},
	journal = {Neuron},
	author = {Summerfield, Jennifer J. and Lepsien, Jöran and Gitelman, Darren R. and Mesulam, M. Marsel and Nobre, Anna C.},
	month = mar,
	year = {2006},
	pmid = {16543137},
	note = {Publisher: Elsevier},
	keywords = {SYSNEURO},
	pages = {905--916},
	file = {Full Text PDF:/Users/luca/Zotero/storage/XL3MM3V4/Summerfield et al. - 2006 - Orienting Attention Based on Long-Term Memory Expe.pdf:application/pdf;Snapshot:/Users/luca/Zotero/storage/9X7QFY45/S0896-6273(06)00074-2.html:text/html},
}

@article{kumle_estimating_2021,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://doi.org/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2022-10-28},
	journal = {Behavior Research Methods},
	author = {Kumle, Levi and Võ, Melissa L.-H. and Draschkow, Dejan},
	month = dec,
	year = {2021},
	keywords = {lme4, Mixed models, mixedpower, Power, R, Simulation},
	pages = {2528--2543},
	file = {Full Text PDF:/Users/luca/Zotero/storage/TBUC98EW/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf:application/pdf},
}

@article{brockmole_using_2006,
	title = {Using real-world scenes as contextual cues for search},
	volume = {13},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/13506280500165188},
	doi = {10.1080/13506280500165188},
	abstract = {Research on contextual cueing has demonstrated that with simple arrays of letters and shapes, search for a target increases in efficiency as associations between a search target and its surrounding visual context are learned. We investigated whether the visual context afforded by repeated exposure to real-world scenes can also guide attention when the relationship between the scene and a target position is arbitrary. Observers searched for and identified a target letter embedded in photographs of real-world scenes. Although search time within novel scenes was consistent across trials, search time within repeated scenes decreased across repetitions. Unlike previous demonstrations of contextual cueing, however, memory for scene-target covariation was explicit. In subsequent memory tests, observers recognized repeated contexts more often than those that were presented once and displayed superior recall of target position within the repeated scenes. In addition, repetition of inverted scenes, which made the scene more difficult to identify, produced a markedly reduced rate of learning, suggesting semantic information concerning object and scene identity are used to guide attention.},
	number = {1},
	urldate = {2022-10-28},
	journal = {Visual Cognition},
	author = {Brockmole, James R. and Henderson, John M.},
	month = jan,
	year = {2006},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13506280500165188},
	pages = {99--108},
}

@article{shaw_anova_1993,
	title = {Anova for {Unbalanced} {Data}: {An} {Overview}},
	volume = {74},
	issn = {1939-9170},
	shorttitle = {Anova for {Unbalanced} {Data}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/1939922},
	doi = {10.2307/1939922},
	abstract = {Ecological studies typically involve comparison of biological responses among a variety of environmental conditions. When the response variables have continuous distributions and the conditions are discrete, whether inherently or by design, then it is appropriate to analyze the data using analysis of variance (ANOVA). When data conform to a complete, balanced design (equal numbers of observations in each experimental treatment), it is straightforward to conduct an ANOVA, particularly with the aid of the numerous statistical computing packages that are available. Interpretation of an ANOVA of balanced data is also unambiguous. Unfortunately, for a variety of reasons, it is rare that a practicing ecologist embarks on an analysis of data that are completely balanced. Regardless of its cause, lack of balance necessitates care in the analysis and interpretation. In this paper, our aims is to provide an overview of the consequences of lack of balance and to give some guidelines to analyzing unbalanced data for models involving fixed effects. Our treatment is necessarily cursory and will not substitute for training available from a sequence of courses in mathematical statistics and linear models. It is intended to introduce the reader to the main issues and to the extensive statistical literature that deals with them.},
	language = {en},
	number = {6},
	urldate = {2022-10-28},
	journal = {Ecology},
	author = {Shaw, Ruth G. and Mitchell-Olds, Thomas},
	year = {1993},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/1939922},
	pages = {1638--1645},
	file = {Snapshot:/Users/luca/Zotero/storage/SZHXPK38/1939922.html:text/html},
}

@article{krueger_comparison_2004,
	title = {A {Comparison} of the {General} {Linear} {Mixed} {Model} and {Repeated} {Measures} {ANOVA} {Using} a {Dataset} with {Multiple} {Missing} {Data} {Points}},
	volume = {6},
	issn = {1099-8004},
	url = {https://doi.org/10.1177/1099800404267682},
	doi = {10.1177/1099800404267682},
	abstract = {Longitudinal methods are the methods of choice for researchers who view their phenomena of interest as dynamic. Although statistical methods have remained largely fixed in a linear view of biology and behavior, more recent methods, such as the general linear mixed model (mixed model), can be used to analyze dynamic phenomena that are often of interest to nurses. Two strengths of the mixed model are (1) the ability to accommodate missing data points often encountered in longitudinal datasets and (2) the ability to model nonlinear, individual characteristics. The purpose of this article is to demonstrate the advantages of using the mixed model for analyzing nonlinear, longitudinal datasets with multiple missing data points by comparing the mixed model to the widely used repeated measures ANOVA using an experimental set of data. The decision-making steps in analyzing the data using both the mixed model and the repeated measures ANOVA are described.},
	language = {en},
	number = {2},
	urldate = {2022-10-28},
	journal = {Biological Research For Nursing},
	author = {Krueger, Charlene and Tian, Lili},
	month = oct,
	year = {2004},
	note = {Publisher: SAGE Publications},
	pages = {151--157},
	file = {SAGE PDF Full Text:/Users/luca/Zotero/storage/SFFS9J88/Krueger und Tian - 2004 - A Comparison of the General Linear Mixed Model and.pdf:application/pdf},
}

@misc{loy__aut_lmeresampler_2022,
	title = {lmeresampler: {Bootstrap} {Methods} for {Nested} {Linear} {Mixed}-{Effects} {Models}},
	copyright = {GPL-3},
	shorttitle = {lmeresampler},
	url = {https://CRAN.R-project.org/package=lmeresampler},
	abstract = {Bootstrap routines for nested linear mixed effects models fit using either 'lme4' or 'nlme'. The provided 'bootstrap()' function implements the parametric, residual, cases, random effect block (REB), and wild bootstrap procedures. An overview of these procedures can be found in Van der Leeden et al. (2008) {\textless}doi:10.1007/978-0-387-73186-5\_11{\textgreater}, Carpenter, Goldstein \& Rasbash (2003) {\textless}doi:10.1111/1467-9876.00415{\textgreater}, and Chambers \& Chandra (2013) {\textless}doi:10.1080/10618600.2012.681216{\textgreater}.},
	urldate = {2022-10-28},
	author = {Loy  [aut, Adam and cre and Steele, Spenser and Korobova, Jenna},
	month = apr,
	year = {2022},
	keywords = {MixedModels},
}

@incollection{chun_scene_2003,
	series = {Cognitive {Vision}},
	title = {Scene {Perception} and {Memory}},
	volume = {42},
	url = {https://www.sciencedirect.com/science/article/pii/S007974210301003X},
	abstract = {The chapter reviews some basic properties of scenes. Despite a lack of consensus on how to operationalize different scenes, visual scenes share a number of properties that are uncontroversial and three of these characteristics are described. The ability to perceive one's local visual environment is so important for navigation and other daily activities that it is perhaps not surprising that a region of the brain appears to be specialized for processing scene information. To understand how such environmental regularities are represented in the brain, it is useful to consider both behavioral and neuroscientific data. Past findings appear to converge to support a dual-path model of scene processing, where global spatial configuration information is rapidly registered and used to guide how a scene is interrogated with multiple eye movements. The parahippocampal cortex responds to visual scenes, namely, depictions of visual space. This region has been dubbed the parahippocampal place area (PPA). As a step toward understanding scene recognition and memory, this chapter also reviews studies from the literature and identifies my laboratory that describe how visual scenes and scene properties are learned and represented in the brain along with the identification of outstanding issues in scene perception and memory that deserve further research.},
	language = {en},
	urldate = {2022-10-29},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {Chun, Marvin M.},
	month = jan,
	year = {2003},
	doi = {10.1016/S0079-7421(03)01003-X},
	pages = {79--108},
	file = {ScienceDirect Snapshot:/Users/luca/Zotero/storage/ZYS4GIM3/S007974210301003X.html:text/html},
}

@article{henderson_high-level_1999,
	title = {High-level scene perception},
	volume = {50},
	issn = {0066-4308},
	url = {https://doi.org/10.1146/annurev.psych.50.1.243},
	doi = {10.1146/annurev.psych.50.1.243},
	abstract = {? Abstract?Three areas of high-level scene perception research are reviewed. The first concerns the role of eye movements in scene perception, focusing on the influence of ongoing cognitive processing on the position and duration of fixations in a scene. The second concerns the nature of the scene representation that is retained across a saccade and other brief time intervals during ongoing scene perception. Finally, we review research on the relationship between scene and object identification, focusing particularly on whether the meaning of a scene influences the identification of constituent objects.},
	number = {1},
	urldate = {2022-10-29},
	journal = {Annual Review of Psychology},
	author = {Henderson, John M. and Hollingworth, Andrew},
	month = feb,
	year = {1999},
	note = {Publisher: Annual Reviews},
	pages = {243--271},
	annote = {doi: 10.1146/annurev.psych.50.1.243},
}

@article{schyns_blobs_1994,
	title = {From blobs to boundary edges: {Evidence} for time- and spatial-scale-dependent scene recognition},
	volume = {5},
	issn = {1467-9280},
	shorttitle = {From blobs to boundary edges},
	doi = {10.1111/j.1467-9280.1994.tb00500.x},
	abstract = {Conducted 2 experiments that contrasted the respective roles of coarse and fine information in fast identification of natural scenes. The 1st experiment, with 20 adults, investigated whether coarse and fine information would be used at different stages of processing. The 2nd experiment, with 20 paid adults, tested whether coarse-to-fine processing would account for fast scene categorization. The data suggest that recognition occurs at both coarse and fine spatial scales. By attending first to the coarse scale, the visual system can get a quick and rough estimate of the input to activate scene schemata in memory; attending to fine information allows refinement or refutation, of the raw estimate. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	journal = {Psychological Science},
	author = {Schyns, Philippe G. and Oliva, Aude},
	year = {1994},
	note = {Place: United Kingdom
Publisher: Blackwell Publishing},
	keywords = {Cognitive Processes, Spatial Frequency, Visual Discrimination},
	pages = {195--200},
	file = {Snapshot:/Users/luca/Zotero/storage/JBLE7JYY/1995-12235-001.html:text/html},
}

@article{efron_better_1987,
	title = {Better {Bootstrap} {Confidence} {Intervals}},
	volume = {82},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478410},
	doi = {10.1080/01621459.1987.10478410},
	number = {397},
	journal = {Journal of the American Statistical Association},
	author = {Efron, Bradley},
	month = mar,
	year = {1987},
	note = {Publisher: Taylor \& Francis},
	pages = {171--185},
	annote = {doi: 10.1080/01621459.1987.10478410},
}

@misc{loy_bootstrapping_2021,
	title = {Bootstrapping {Clustered} {Data} in {R} using lmeresampler},
	url = {http://arxiv.org/abs/2106.06568},
	doi = {10.48550/arXiv.2106.06568},
	abstract = {Linear mixed-effects models are commonly used to analyze clustered data structures. There are numerous packages to fit these models in R and conduct likelihood-based inference. The implementation of resampling-based procedures for inference are more limited. In this paper, we introduce the lmeresampler package for bootstrapping nested linear mixed-effects models fit via lme4 or nlme. Bootstrap estimation allows for bias correction, adjusted standard errors and confidence intervals for small samples sizes and when distributional assumptions break down. We will also illustrate how bootstrap resampling can be used to diagnose this model class. In addition, lmeresampler makes it easy to construct interval estimates of functions of model parameters.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Loy, Adam and Korobova, Jenna},
	month = jun,
	year = {2021},
	note = {arXiv:2106.06568 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
	annote = {Comment: 15 pages, 3 figures, 2 tables,},
	file = {arXiv Fulltext PDF:/Users/luca/Zotero/storage/NRDPX3F5/Loy und Korobova - 2021 - Bootstrapping Clustered Data in R using lmeresampl.pdf:application/pdf;arXiv.org Snapshot:/Users/luca/Zotero/storage/J99ZNKXH/2106.html:text/html},
}

@article{conci_contextual_2012,
	title = {Contextual learning of multiple target locations in visual search},
	volume = {20},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/13506285.2012.694376},
	doi = {10.1080/13506285.2012.694376},
	abstract = {In visual search, detection of a target is faster when a layout of nontarget items is repeatedly encountered, suggesting that contextual invariances can guide attention. Moreover, contextual cueing can also adapt to environmental changes. For instance, when the target undergoes a predictable (i.e., learnable) location change, then contextual cueing remains effective even after the change, suggesting that a learned context is “remapped” and adjusted to novel requirements. Here, we explored the stability of contextual remapping: Four experiments demonstrated that target location changes are only effectively remapped when both the initial and the future target positions remain predictable across the entire experiment. Otherwise, contextual remapping fails. In sum, this pattern of results suggests that multiple, predictable target locations can be associated with a given repeated context, allowing the flexible adaptation of previously learned contingencies to novel task demands.},
	number = {7},
	urldate = {2022-11-01},
	journal = {Visual Cognition},
	author = {Conci, Markus and Müller, Hermann J.},
	month = aug,
	year = {2012},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13506285.2012.694376},
	keywords = {Implicit learning, Contextual cueing, Visual search},
	pages = {746--770},
}

@article{kumle_estimating_2021-1,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://doi.org/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2022-11-02},
	journal = {Behavior Research Methods},
	author = {Kumle, Levi and Võ, Melissa L.-H. and Draschkow, Dejan},
	month = dec,
	year = {2021},
	keywords = {lme4, Mixed models, mixedpower, Power, R, Simulation},
	pages = {2528--2543},
	file = {Full Text PDF:/Users/luca/Zotero/storage/PLJB955V/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf:application/pdf},
}

@misc{allaire_rmarkdown_2022,
	title = {rmarkdown: {Dynamic} {Documents} for {R}},
	copyright = {GPL-3},
	shorttitle = {rmarkdown},
	url = {https://CRAN.R-project.org/package=rmarkdown},
	abstract = {Convert R Markdown documents into a variety of formats.},
	urldate = {2022-11-02},
	author = {Allaire, J. J. and Xie  [aut, Yihui and cre and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard and Dunning, Andrew and filter), Atsushi Yasumoto (Number sections Lua and Schloerke, Barret and Sievert, Carson and Dervieux, Christophe and Ryan, Devon and Aust, Frederik and Allen, Jeff and Seo, JooYoung and Barrett, Malcolm and Hyndman, Rob and Lesur, Romain and Storey, Roy and Arslan, Ruben and Oller, Sergio and RStudio and PBC and inst/rmd/h/jqueryui-AUTHORS.txt), jQuery UI contributors (jQuery UI library; authors listed in and library), Mark Otto (Bootstrap and library), Jacob Thornton (Bootstrap and library), Bootstrap contributors (Bootstrap and Twitter and library), Inc (Bootstrap and library), Alexander Farkas (html5shiv and library), Scott Jehl (Respond js and library), Ivan Sagalaev (highlight js and library), Greg Franko (tocify and templates), John MacFarlane (Pandoc and Google and library), Inc (ioslides and library), Dave Raggett (slidy and library), W3C (slidy and Gandy  (Font-Awesome), Dave and Sperry  (Ionicons), Ben and Drifty (Ionicons) and StickyTabs), Aidan Lister (jQuery and filter), Benct Philip Jonsson (pagebreak Lua and filter), Albert Krewinkel (pagebreak Lua},
	month = oct,
	year = {2022},
	keywords = {ReproducibleResearch},
}
