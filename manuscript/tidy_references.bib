
@article{baayen_mixed-effects_2008,
	title = {Mixed-effects modeling with crossed random effects for subjects and items},
	volume = {59},
	issn = {0749-596X},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X07001398},
	doi = {10.1016/j.jml.2007.12.005},
	abstract = {This paper provides an introduction to mixed-effects models for the analysis of repeated measurement data with subjects and items as crossed random effects. A worked-out example of how to use recent software for mixed-effects modeling is provided. Simulation studies illustrate the advantages offered by mixed-effects analyses compared to traditional analyses based on quasi-F tests, by-subjects analyses, combined by-subjects and by-items analyses, and random regression. Applications and possibilities across a range of domains of inquiry are discussed.},
	number = {4},
	journal = {Special Issue: Emerging Data Analysis},
	author = {Baayen, R.H. and Davidson, D.J. and Bates, D.M.},
	month = nov,
	year = {2008},
	keywords = {By-item, By-subject, Crossed random effects, Mixed-effects models, Quasi-F, notion},
	pages = {390--412},
}

@article{chun_contextual_1998,
  title = {Contextual {Cueing}: {Implicit} {Learning} and {Memory} of {Visual} {Context} {Guides} {Spatial} {Attention}},
  volume = {36},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028598906818},
  doi = {10.1006/cogp.1998.0681},
  number = {1},
  journal = {Cognitive Psychology},
  author = {Marvin M. Chun and Yuhong Jiang},
  month = {jun},
  year = {1998},
  pages = {28--71},
}

@article{chun_contextual_2000,
  title = {Contextual cueing of visual attention},
  volume = {4},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661300014765},
  doi = {10.1016/S1364-6613(00)01476-5},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  author = {Marvin M. Chun},
  month = {may},
  year = {2000},
  pages = {170--178},
}

@article{makovski_contextual_2010,
  title = {Contextual cost: {When} a visual-search target is not where it should be},
  volume = {63},
  url = {https://doi.org/10.1080/17470210903281590},
  doi = {10.1080/17470210903281590},
  number = {2},
  journal = {Quarterly Journal of Experimental Psychology},
  author = {Tal Makovski and Yuhong V. Jiang},
  month = {feb},
  year = {2010},
  note = {Publisher: SAGE Publications},
  pages = {216--225},
  annote = {doi: 10.1080/17470210903281590},
}

@article{brockmole_recognition_2006,
  title = {Recognition and attention guidance during contextual cueing in real-world scenes: {Evidence} from eye movements},
  volume = {59},
  url = {https://doi.org/10.1080/17470210600665996},
  doi = {10.1080/17470210600665996},
  number = {7},
  journal = {The Quarterly Journal of Experimental Psychology},
  author = {James R. Brockmole and John M. Henderson},
  month = {jul},
  year = {2006},
  note = {Publisher: Routledge},
  pages = {1177--1187},
  annote = {doi: 10.1080/17470210600665996},
}

@article{manginelli_misleading_2009,
  title = {Misleading contextual cues: {How} do they affect visual search?},
  volume = {73},
  url = {https://doi.org/10.1007/s00426-008-0211-1},
  doi = {10.1007/s00426-008-0211-1},
  number = {2},
  journal = {Psychological Research},
  author = {Angela A. Manginelli and Stefan Pollmann},
  month = {mar},
  year = {2009},
  pages = {212--221},
}

@article{zellin_here_2013,
  title = {Here {Today}, {Gone} {Tomorrow} – {Adaptation} to {Change} in {Memory}-{Guided} {Visual} {Search}},
  volume = {8},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0059466},
  doi = {10.1371/journal.pone.0059466},
  number = {3},
  journal = {PLOS ONE},
  author = {Martina Zellin and Markus Conci and Adrian von M{\"u}hlenen and Hermann J. M{\"u}ller},
  month = {mar},
  year = {2013},
  note = {Publisher: Public Library of Science},
  pages = {e59466},
}

@book{hox_multilevel_2017,
  address = {New York},
  edition = {3},
  title = {Multilevel {Analysis}: {Techniques} and {Applications}},
  publisher = {Routledge},
  author = {Joop Hox and Mirjam Moerbeek and Rens van de Schoot},
  month = {sep},
  year = {2017},
  doi = {10.4324/9781315650982},
}

@article{peirce_psychopy2_2019,
  title = {{PsychoPy2}: {Experiments} in behavior made easy},
  volume = {51},
  url = {https://doi.org/10.3758/s13428-018-01193-y},
  doi = {10.3758/s13428-018-01193-y},
  number = {1},
  journal = {Behavior Research Methods},
  author = {Jonathan Peirce and Jeremy R. Gray and Sol Simpson and Michael MacAskill and Richard H{\"o}chenberger and Hiroyuki Sogo and Erik Kastman and Jonas Kristoffer Lindel{\o}v},
  month = {feb},
  year = {2019},
  pages = {195--203},
  note = {Last visited on 10/17/2022},
}

@misc{r_core_team_r_2022,
  address = {Vienna, Austria},
  title = {R: {A} language and   environment for statistical computing},
  url = {https://www.R-project.org/},
  publisher = {R Foundation for Statistical Computing},
  author = {R Core Team},
  year = {2022},
}

@misc{rstudio_team_rstudio_2022,
  address = {Boston, MA},
  title = {{RStudio}: {Integrated} {Development} {Environment}   for {R}},
  url = {http://www.rstudio.com/.},
  publisher = {RStudio, PBC},
  author = {RStudio Team},
  year = {2022},
}

@misc{bates_lme4_2022,
  title = {lme4: {Linear} {Mixed}-{Effects} {Models} using '{Eigen}' and {S4}},
  url = {https://CRAN.R-project.org/package=lme4},
  author = {Douglas Bates and Martin Maechler and Ben Bolker and Steven Walker and Rune Haubo Bojesen Christensen and Henrik Singmann and Bin Dai and Fabian Scheipl and Gabor Grothendieck and Peter Green and John Fox and Alexander Bauer and Pavel N. Krivitsky},
  month = {jul},
  year = {2022},
  note = {Last visited on 10/17/2022},
}

@article{meteyard_best_2020,
  title = {Best practice guidance for linear mixed-effects models in psychological science},
  volume = {112},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300061},
  doi = {10.1016/j.jml.2020.104092},
  journal = {Journal of Memory and Language},
  author = {Lotte Meteyard and Robert A.I. Davies},
  month = {jun},
  year = {2020},
  pages = {104092},
}

@article{mohr_boisberlin_2016,
  title = {{BOiS}—{Berlin} {Object} in {Scene} {Database}: {Controlled} {Photographic} {Images} for {Visual} {Search} {Experiments} with {Quantified} {Contextual} {Priors}},
  volume = {7},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.00749},
  journal = {Frontiers in Psychology},
  author = {Johannes Mohr and Julia Seyfarth and Andreas Lueschow and Joachim E. Weber and Felix A. Wichmann and Klaus Obermayer},
  year = {2016},
}

@article{harrison_brief_2018,
  title = {A brief introduction to mixed effects modelling and multi-model inference in ecology},
  volume = {6},
  doi = {10.7717/peerj.4794},
  journal = {PeerJ},
  author = {Xavier Harrison and Lynda Donaldson and Maru Correa and Julian Evans and David Fisher and Cecily Goodwin and Beth Robinson and David Hodgson and Richard Inger},
  month = {may},
  year = {2018},
  pages = {e4794},
}

@article{bono_report_2021,
  title = {Report {Quality} of {Generalized} {Linear} {Mixed} {Models} in {Psychology}: {A} {Systematic} {Review}},
  volume = {12},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.666182},
  journal = {Frontiers in Psychology},
  author = {Roser Bono and Rafael Alarc{\a'o}n and Mar{\a'\i}a J. Blanca},
  year = {2021},
  note = {Last visited on 10/21/2022},
}

@article{blanca_current_2018,
  title = {Current {Practices} in {Data} {Analysis} {Procedures} in {Psychology}: {What} {Has} {Changed}?},
  volume = {9},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02558},
  journal = {Frontiers in Psychology},
  author = {Mar{\a'\i}a J. Blanca and Rafael Alarc{\a'o}n and Roser Bono},
  year = {2018},
  note = {Last visited on 10/21/2022},
}

@article{brockmole_contextual_2006,
  title = {Contextual cueing in naturalistic scenes: {Global} and local contexts},
  volume = {32},
  doi = {10.1037/0278-7393.32.4.699},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  author = {James R. Brockmole and Monica S. Castelhano and John M. Henderson},
  year = {2006},
  note = {Place: US
Publisher: American Psychological Association},
  pages = {699--706},
}

@article{rosenbaum_interaction_2013,
  title = {Interaction between scene-based and array-based contextual cueing},
  volume = {75},
  url = {https://doi.org/10.3758/s13414-013-0446-9},
  doi = {10.3758/s13414-013-0446-9},
  number = {5},
  journal = {Attention, Perception, \& Psychophysics},
  author = {Gail M. Rosenbaum and Yuhong V. Jiang},
  month = {jul},
  year = {2013},
  pages = {888--899},
  note = {Last visited on 10/26/2022},
}

@article{henderson_high-level_1999,
	title = {High-level scene perception},
	volume = {50},
	issn = {0066-4308},
	url = {https://doi.org/10.1146/annurev.psych.50.1.243},
	doi = {10.1146/annurev.psych.50.1.243},
	abstract = {? Abstract?Three areas of high-level scene perception research are reviewed. The first concerns the role of eye movements in scene perception, focusing on the influence of ongoing cognitive processing on the position and duration of fixations in a scene. The second concerns the nature of the scene representation that is retained across a saccade and other brief time intervals during ongoing scene perception. Finally, we review research on the relationship between scene and object identification, focusing particularly on whether the meaning of a scene influences the identification of constituent objects.},
	number = {1},
	urldate = {2022-10-29},
	journal = {Annual Review of Psychology},
	author = {Henderson, John M. and Hollingworth, Andrew},
	month = feb,
	year = {1999},
	note = {Publisher: Annual Reviews},
	pages = {243--271},
	annote = {doi: 10.1146/annurev.psych.50.1.243},
}

@incollection{jiang_contextual_2020,
	address = {New York, NY},
	title = {Contextual {Cueing}},
	isbn = {978-1-4939-9948-4},
	url = {https://doi.org/10.1007/7657_2019_19},
	abstract = {Contextual cueing refers to the facilitation of visual search by the occasional repetition of a visual context. In standard spatial contextual cueing, a visual search target appears in a consistent location within a repeated array of objects. Search time is faster on repeated displays relative to novel displays, even in participants who do not explicitly recognize the repeated displays. Contextual cueing exemplifies the importance of statistical learning and the resulting memory in guiding attention. Because it involves implicit, relational learning, it has been instrumental in understanding brain functions, cognitive changes across the life span, as well as effects of various neurological, neurodevelopmental, and psychiatric conditions. To stimulate further behavioral and brain research on memory-guided attention and to facilitate comparisons across studies, here we provide a methodological guide on the experimental paradigm of contextual cueing and review key findings. We identify factors that influence the strength of the effect and underscore potential pitfalls in experimental design, data analysis, and interpretation.},
	booktitle = {Spatial {Learning} and {Attention} {Guidance}},
	publisher = {Springer US},
	author = {Jiang, Yuhong V. and Sisk, Caitlin A.},
	editor = {Pollmann, Stefan},
	year = {2020},
	doi = {10.1007/7657_2019_19},
	pages = {59--72},
}

@article{goujon_investigating_2015,
  title = {Investigating implicit statistical learning mechanisms through contextual cueing},
  volume = {19},
  doi = {10.1016/j.tics.2015.07.009},
  number = {9},
  journal = {Trends in Cognitive Sciences},
  author = {Annabelle Goujon and Andr{\a'e} Didierjean and Simon Thorpe},
  month = {sep},
  year = {2015},
  pages = {524--533},
}

@article{conci_contextual_2011,
  title = {Contextual remapping in visual search after predictable target-location changes},
  volume = {75},
  url = {https://doi.org/10.1007/s00426-010-0306-3},
  doi = {10.1007/s00426-010-0306-3},
  number = {4},
  journal = {Psychological Research},
  author = {Markus Conci and Luning Sun and Hermann J. M{\"u}ller},
  month = {jul},
  year = {2011},
  pages = {279--289},
  note = {Last visited on 10/26/2022},
}

@article{oliva_role_2007,
  title = {The role of context in object recognition},
  volume = {11},
  doi = {10.1016/j.tics.2007.09.009},
  journal = {Trends in Cognitive Sciences},
  author = {Aude Oliva and Antonio Torralba},
  year = {2007},
  note = {Place: Netherlands
Publisher: Elsevier Science},
  pages = {520--527},
}

@book{peirce_building_2022,
  edition = {2},
  title = {Building {Experiments} in {PsychoPy}},
  publisher = {Sage},
  author = {J. W. Peirce and R. J. Hirst and M. R. MacAskill},
  year = {2022},
}

@incollection{jiang_contextual_2020,
  address = {New York, NY},
  title = {Contextual {Cueing}},
  url = {https://doi.org/10.1007/7657_2019_19},
  booktitle = {Spatial {Learning} and {Attention} {Guidance}},
  publisher = {Springer US},
  author = {Yuhong V. Jiang and Caitlin A. Sisk},
  editor = {Stefan Pollmann},
  year = {2020},
  doi = {10.1007/7657_2019_19},
  pages = {59--72},
}

@article{brockmole_using_2006,
  title = {Using real-world scenes as contextual cues for search},
  volume = {13},
  url = {https://doi.org/10.1080/13506280500165188},
  doi = {10.1080/13506280500165188},
  number = {1},
  journal = {Visual Cognition},
  author = {James R. Brockmole and John M. Henderson},
  month = {jan},
  year = {2006},
  note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13506280500165188},
  pages = {99--108},
}

@article{shaw_anova_1993,
  title = {Anova for {Unbalanced} {Data}: {An} {Overview}},
  volume = {74},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/1939922},
  doi = {10.2307/1939922},
  number = {6},
  journal = {Ecology},
  author = {Ruth G. Shaw and Thomas Mitchell-Olds},
  year = {1993},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/1939922},
  pages = {1638--1645},
}

@article{krueger_comparison_2004,
  title = {A {Comparison} of the {General} {Linear} {Mixed} {Model} and {Repeated} {Measures} {ANOVA} {Using} a {Dataset} with {Multiple} {Missing} {Data} {Points}},
  volume = {6},
  url = {https://doi.org/10.1177/1099800404267682},
  doi = {10.1177/1099800404267682},
  number = {2},
  journal = {Biological Research For Nursing},
  author = {Charlene Krueger and Lili Tian},
  month = {oct},
  year = {2004},
  note = {Publisher: SAGE Publications},
  pages = {151--157},
}

@incollection{chun_scene_2003,
  series = {Cognitive {Vision}},
  title = {Scene {Perception} and {Memory}},
  volume = {42},
  url = {https://www.sciencedirect.com/science/article/pii/S007974210301003X},
  booktitle = {Psychology of {Learning} and {Motivation}},
  publisher = {Academic Press},
  author = {Marvin M. Chun},
  month = {jan},
  year = {2003},
  doi = {10.1016/S0079-7421(03)01003-X},
  pages = {79--108},
  note = {Last visited on 10/29/2022},
}

@misc{loy_bootstrapping_2021,
  title = {Bootstrapping {Clustered} {Data} in {R} using lmeresampler},
  url = {http://arxiv.org/abs/2106.06568},
  doi = {10.48550/arXiv.2106.06568},
  publisher = {arXiv},
  author = {Adam Loy and Jenna Korobova},
  month = {jun},
  year = {2021},
  note = {arXiv:2106.06568 [stat]},
  annote = {Comment: 15 pages, 3 figures, 2 tables,},
}

@article{conci_contextual_2012,
	title = {Contextual learning of multiple target locations in visual search},
	volume = {20},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/13506285.2012.694376},
	doi = {10.1080/13506285.2012.694376},
	abstract = {In visual search, detection of a target is faster when a layout of nontarget items is repeatedly encountered, suggesting that contextual invariances can guide attention. Moreover, contextual cueing can also adapt to environmental changes. For instance, when the target undergoes a predictable (i.e., learnable) location change, then contextual cueing remains effective even after the change, suggesting that a learned context is “remapped” and adjusted to novel requirements. Here, we explored the stability of contextual remapping: Four experiments demonstrated that target location changes are only effectively remapped when both the initial and the future target positions remain predictable across the entire experiment. Otherwise, contextual remapping fails. In sum, this pattern of results suggests that multiple, predictable target locations can be associated with a given repeated context, allowing the flexible adaptation of previously learned contingencies to novel task demands.},
	number = {7},
	urldate = {2022-11-01},
	journal = {Visual Cognition},
	author = {Conci, Markus and Müller, Hermann J.},
	month = aug,
	year = {2012},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/13506285.2012.694376},
	keywords = {Contextual cueing, Implicit learning, Visual search},
	pages = {746--770},
}

@article{kumle_estimating_2021,
	title = {Estimating power in (generalized) linear mixed models: {An} open introduction and tutorial in {R}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Estimating power in (generalized) linear mixed models},
	url = {https://doi.org/10.3758/s13428-021-01546-0},
	doi = {10.3758/s13428-021-01546-0},
	abstract = {Mixed-effects models are a powerful tool for modeling fixed and random effects simultaneously, but do not offer a feasible analytic solution for estimating the probability that a test correctly rejects the null hypothesis. Being able to estimate this probability, however, is critical for sample size planning, as power is closely linked to the reliability and replicability of empirical findings. A flexible and very intuitive alternative to analytic power solutions are simulation-based power analyses. Although various tools for conducting simulation-based power analyses for mixed-effects models are available, there is lack of guidance on how to appropriately use them. In this tutorial, we discuss how to estimate power for mixed-effects models in different use cases: first, how to use models that were fit on available (e.g. published) data to determine sample size; second, how to determine the number of stimuli required for sufficient power; and finally, how to conduct sample size planning without available data. Our examples cover both linear and generalized linear models and we provide code and resources for performing simulation-based power analyses on openly accessible data sets. The present work therefore helps researchers to navigate sound research design when using mixed-effects models, by summarizing resources, collating available knowledge, providing solutions and tools, and applying them to real-world problems in sample sizing planning when sophisticated analysis procedures like mixed-effects models are outlined as inferential procedures.},
	language = {en},
	number = {6},
	urldate = {2022-11-02},
	journal = {Behavior Research Methods},
	author = {Kumle, Levi and Võ, Melissa L.-H. and Draschkow, Dejan},
	month = dec,
	year = {2021},
	keywords = {lme4, Mixed models, mixedpower, Power, R, Simulation},
	pages = {2528--2543},
	file = {Full Text PDF:/Users/luca/Zotero/storage/PLJB955V/Kumle et al. - 2021 - Estimating power in (generalized) linear mixed mod.pdf:application/pdf},
}

@misc{wickham_dplyr_2022,
	title = {dplyr: {A} {Grammar} of {Data} {Manipulation}},
	copyright = {MIT + file LICENSE},
	shorttitle = {dplyr},
	url = {https://CRAN.R-project.org/package=dplyr},
	abstract = {A fast, consistent tool for working with data frame like objects, both in memory and out of memory.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and François, Romain and Henry, Lionel and Müller, Kirill},
	month = sep,
	year = {2022},
	keywords = {Databases, ModelDeployment, Epidemiology},
}

@misc{henry_purrr_2022,
	title = {purrr: {Functional} {Programming} {Tools}},
	copyright = {GPL-3 {\textbar} file LICENSE},
	shorttitle = {purrr},
	url = {https://CRAN.R-project.org/package=purrr},
	abstract = {A complete and consistent functional programming toolkit for R.},
	urldate = {2022-10-17},
	author = {Henry, Lionel and Wickham, Hadley},
	month = oct,
	year = {2022},
}


@misc{kuhn_multilevelmod_2022,
	title = {multilevelmod: {Model} {Wrappers} for {Multi}-{Level} {Models}},
	copyright = {MIT + file LICENSE},
	shorttitle = {multilevelmod},
	url = {https://CRAN.R-project.org/package=multilevelmod},
	abstract = {Bindings for hierarchical regression models for use with the 'parsnip' package. Models include longitudinal generalized linear models (Liang and Zeger, 1986) {\textless}doi:10.1093/biomet/73.1.13{\textgreater}, and mixed-effect models (Pinheiro and Bates) {\textless}doi:10.1007/978-1-4419-0318-1\_1{\textgreater}.},
	urldate = {2022-10-17},
	author = {Kuhn, Max and Frick, Hannah and RStudio},
	month = jun,
	year = {2022},
}


@misc{wickham_stringr_2022,
	title = {stringr: {Simple}, {Consistent} {Wrappers} for {Common} {String} {Operations}},
	copyright = {GPL-2 {\textbar} file LICENSE},
	shorttitle = {stringr},
	url = {https://CRAN.R-project.org/package=stringr},
	abstract = {A consistent, simple and easy to use set of wrappers around the fantastic 'stringi' package. All function and argument names (and positions) are consistent, all functions deal with "NA"'s and zero length vectors in the same way, and the output from one function is easy to feed into the input of another.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley},
	month = aug,
	year = {2022},
}

@misc{kuhn_tidymodels_2022,
	title = {tidymodels: {Easily} {Install} and {Load} the '{Tidymodels}' {Packages}},
	copyright = {MIT + file LICENSE},
	shorttitle = {tidymodels},
	url = {https://CRAN.R-project.org/package=tidymodels},
	abstract = {The tidy modeling "verse" is a collection of packages for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.},
	urldate = {2022-10-17},
	author = {Kuhn, Max and Wickham, Hadley},
	month = jul,
	year = {2022},
	keywords = {MachineLearning},
}

@misc{bolker_broommixed_2022,
	title = {broom.mixed: {Tidying} {Methods} for {Mixed} {Models}},
	copyright = {GPL-3},
	shorttitle = {broom.mixed},
	url = {https://CRAN.R-project.org/package=broom.mixed},
	abstract = {Convert fitted objects from various R mixed-model packages into tidy data frames along the lines of the 'broom' package. The package provides three S3 generics for each model: tidy(), which summarizes a model's statistical findings such as coefficients of a regression; augment(), which adds columns to the original data such as predictions, residuals and cluster assignments; and glance(), which provides a one-row summary of model-level statistics.},
	urldate = {2022-10-17},
	author = {Bolker, Ben and Robinson, David and Menne, Dieter and Gabry, Jonah and Buerkner, Paul and Hua, Christopher and Petry, William and Wiley, Joshua and Kennedy, Patrick and Szöcs, Szöcs and Patil, Indrajeet and Arel-Bundock, Vincent and Denney, Bill and Brunson, Cory},
	month = apr,
	year = {2022},
}

@misc{wickham_ggplot2_2022,
	title = {ggplot2: {Create} {Elegant} {Data} {Visualisations} {Using} the {Grammar} of {Graphics}},
	copyright = {MIT + file LICENSE},
	shorttitle = {ggplot2},
	url = {https://CRAN.R-project.org/package=ggplot2},
	abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
	urldate = {2022-10-17},
	author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
	month = may,
	year = {2022},
	keywords = {TeachingStatistics, Spatial},
}

@misc{wilke_ggridges_2022,
	title = {ggridges: {Ridgeline} {Plots} in 'ggplot2'},
	copyright = {GPL-2 {\textbar} file LICENSE},
	shorttitle = {ggridges},
	url = {https://CRAN.R-project.org/package=ggridges},
	abstract = {Ridgeline plots provide a convenient way of visualizing changes in distributions over time or space. This package enables the creation of such plots in 'ggplot2'.},
	urldate = {2022-10-17},
	author = {Wilke, Claus O.},
	month = sep,
	year = {2022},
}

@misc{kay_ggdist_2022,
	title = {ggdist: {Visualizations} of {Distributions} and {Uncertainty}},
	copyright = {GPL (≥ 3)},
	shorttitle = {ggdist},
	url = {https://CRAN.R-project.org/package=ggdist},
	abstract = {Provides primitives for visualizing distributions using 'ggplot2' that are particularly tuned for visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as bootstrap distributions or Bayesian posterior samples) are easily visualized. Visualization primitives include but are not limited to: points with multiple uncertainty intervals, eye plots (Spiegelhalter D., 1999) {\textless}https://ideas.repec.org/a/bla/jorssa/v162y1999i1p45-58.html{\textgreater}, density plots, gradient plots, dot plots (Wilkinson L., 1999) {\textless}doi:10.1080/00031305.1999.10474474{\textgreater}, quantile dot plots (Kay M., Kola T., Hullman J., Munson S., 2016) {\textless}doi:10.1145/2858036.2858558{\textgreater}, complementary cumulative distribution function barplots (Fernandes M., Walls L., Munson S., Hullman J., Kay M., 2018) {\textless}doi:10.1145/3173574.3173718{\textgreater}, and fit curves with multiple uncertainty ribbons.},
	urldate = {2022-10-17},
	author = {Kay, Matthew and Wiernik, Brenton M.},
	month = jul,
	year = {2022},
}

@misc{tiedemann_gghalves_2022,
	title = {gghalves: {Compose} {Half}-{Half} {Plots} {Using} {Your} {Favourite} {Geoms}},
	copyright = {MIT + file LICENSE},
	shorttitle = {gghalves},
	url = {https://CRAN.R-project.org/package=gghalves},
	abstract = {A 'ggplot2' extension for easy plotting of half-half geom combinations. Think half boxplot and half jitterplot, or half violinplot and half dotplot.},
	urldate = {2022-10-17},
	author = {Tiedemann, Frederik},
	month = may,
	year = {2022},
}

@misc{wilke_ggtext_2022,
	title = {ggtext: {Improved} {Text} {Rendering} {Support} for 'ggplot2'},
	copyright = {GPL-2},
	shorttitle = {ggtext},
	url = {https://CRAN.R-project.org/package=ggtext},
	abstract = {A 'ggplot2' extension that enables the rendering of complex formatted plot labels (titles, subtitles, facet labels, axis labels, etc.). Text boxes with automatic word wrap are also supported.},
	urldate = {2022-10-17},
	author = {Wilke, Claus O. and Wiernik, Brenton M.},
	month = sep,
	year = {2022},
}


@misc{aust_papaja_2022,
	title = {papaja: {Prepare} {American} {Psychological} {Association} {Journal} {Articles} with {R} {Markdown}},
	copyright = {MIT + file LICENSE},
	shorttitle = {papaja},
	url = {https://CRAN.R-project.org/package=papaja},
	abstract = {Tools to create dynamic, submission-ready manuscripts, which conform to American Psychological Association manuscript guidelines. We provide R Markdown document formats for manuscripts (PDF and Word) and revision letters (PDF). Helper functions facilitate reporting statistical analyses or create publication-ready tables and plots.},
	urldate = {2022-10-25},
	author = {Aust, Frederik and Barth, Marius and Diedenhofen, Birk and Stahl, Christoph and Casillas, Joseph V. and Siegel, Rudolf},
	month = jul,
	year = {2022},
}


@misc{xie_knitr_2022,
	title = {knitr: {A} {General}-{Purpose} {Package} for {Dynamic} {Report} {Generation} in {R}},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL]},
	shorttitle = {knitr},
	url = {https://CRAN.R-project.org/package=knitr},
	abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
	urldate = {2022-10-25},
	author = {Xie, Yihui and Sarma, Abhraneel and Vogt, Adam and Andrew, Alastair and Zvoleff, Alex and Al-Zubaidi, Amar and Simon, Andre and Atkins, Aron and Wolen, Aaron and Manton, Ashley and Yasumoto, Atsushi and Baumer, Ben and Diggs, Brian and Zhang, Brian and Yapparov, Bulat and Pereira, Cassio and Dervieux, Christophe and Hall, David and Hugh-Jones, David and Robinson, David and Hemken, Doug and Murdoch, Duncan and Campitelli, Elio and Hughes, Ellis and Riederer, Emily and Hirschmann, Fabian and Simeon, Fitch and Fang, Forest and Harrell Jr, Frank E. and Aden-Buie, Garrick and Detrez, Gregoire and Wickham, Hadley and Zhu, Hao and Jeon, Heewon and Bengtsson, Henrik and Yutani, Hiroaki and Lyttle, Ian and Daniel, Hodges and Bien, Jacob and Burkhead, Jake and Manton, James and Lander, Jared and Punyon, Jason and Luraschi, Javier and Arnold, Jeff and Bryan, Jenny and Ashkenas, Jeremy and Stephens, Jeremy and Hester, Jim and Cheng, Joe and Ranke, Johannes and Honaker, John and Muschelli, John and Keane, Jonathan and Allaire, J. J. and Toloe, Johan and Sidi, Jonathan and Larmarange, Joseph and Barnier, Julien and Zhong, Kaiyin and Slowikowski, Kamil and Forner, Karl and Smith, Kevin K. and Mueller, Kirill and Takahashi, Kohske and Walthert, Lorenz and Gallindo, Lucas and Hofert, Marius and Modrák, Martin and Chirico, Michael and Friendly, Michael and Bojanowski, Michal and Kuhlmann, Michel and Patrick, Miller and Caballero, Nacho and Salkowski, Nick and Hansen, Niels Richard and Ross, Noam and Mahdi, Obada and Krivitsky, Pavel N. and Li, Qiang and Vaidyanathan, Ramnath and Cotton, Richard and Krzyzanowski, Robert and Copetti, Rodrigo and Francois, Romain and Williamson, Ruaridh and Mati, Sagiru and Kostyshak, Scott and Meyer, Sebastian and Brouwer, Sietse and Bernard, Simon de and Rousseau, Sylvain and Wei, Taiyun and Assus, Thibaut and Lamadon, Thibaut and Leeper, Thomas and Mastny, Tim and Torsney-Weir, Tom and Davis, Trevor and Veitas, Viktoras and Zhu, Weicheng and Wu, Wush and Foster, Zachary and Kamvar, Zhian N.},
	month = aug,
	year = {2022},
	keywords = {ReproducibleResearch},
}

@misc{allaire_rmarkdown_2022,
	title = {rmarkdown: {Dynamic} {Documents} for {R}},
	copyright = {GPL-3},
	shorttitle = {rmarkdown},
	url = {https://CRAN.R-project.org/package=rmarkdown},
	abstract = {Convert R Markdown documents into a variety of formats.},
	urldate = {2022-11-02},
	author = {Allaire, J. J. and Xie  [aut, Yihui and cre and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard and Dunning, Andrew and filter), Atsushi Yasumoto (Number sections Lua and Schloerke, Barret and Sievert, Carson and Dervieux, Christophe and Ryan, Devon and Aust, Frederik and Allen, Jeff and Seo, JooYoung and Barrett, Malcolm and Hyndman, Rob and Lesur, Romain and Storey, Roy and Arslan, Ruben and Oller, Sergio and RStudio and PBC and inst/rmd/h/jqueryui-AUTHORS.txt), jQuery UI contributors (jQuery UI library; authors listed in and library), Mark Otto (Bootstrap and library), Jacob Thornton (Bootstrap and library), Bootstrap contributors (Bootstrap and Twitter and library), Inc (Bootstrap and library), Alexander Farkas (html5shiv and library), Scott Jehl (Respond js and library), Ivan Sagalaev (highlight js and library), Greg Franko (tocify and templates), John MacFarlane (Pandoc and Google and library), Inc (ioslides and library), Dave Raggett (slidy and library), W3C (slidy and Gandy  (Font-Awesome), Dave and Sperry  (Ionicons), Ben and Drifty (Ionicons) and StickyTabs), Aidan Lister (jQuery and filter), Benct Philip Jonsson (pagebreak Lua and filter), Albert Krewinkel (pagebreak Lua},
	month = oct,
	year = {2022},
	keywords = {ReproducibleResearch},
}
